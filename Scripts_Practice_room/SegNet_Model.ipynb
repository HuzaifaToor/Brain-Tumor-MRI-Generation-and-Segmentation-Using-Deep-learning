{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SegNet_Model.ipynb","provenance":[],"mount_file_id":"1B-lQm9YYX4Xe2qRZmwqV-IEQgdNMn8Pt","authorship_tag":"ABX9TyOd4FE4fUFNo/owvybZc5vl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gEOKWqxVS2fR"},"source":["######################################\n","\n","\n","# -*- coding: utf-8 -*-\n","\"\"\"SegNet model for Keras.\n","# Reference:\n","- [Segnet: A deep convolutional encoder-decoder architecture for image segmentation](https://arxiv.org/pdf/1511.00561.pdf)\n","\"\"\"\n","\n","from __future__ import absolute_import\n","from __future__ import print_function\n","import os\n","import numpy as np\n","from keras.utils import np_utils\n","from keras.applications import imagenet_utils\n","\n","\n","########################\n","from keras.models import Model\n","from keras.layers import Input\n","from keras.layers.core import Activation, Reshape\n","from keras.layers import BatchNormalization\n","import tensorflow as tf\n","from torch.nn import MaxUnpool3d\n","from keras.layers import Conv3D, MaxPooling3D, concatenate, UpSampling3D\n","\n","\n","def SegNet(input_shape, classes):\n","    kernel=(3, 3, 3)\n","    pool_size=(2, 2, 2)\n","    output_mode=\"softmax\"\n","    \n","    img_input = Input(shape=input_shape)\n","    x = img_input\n","    # Encoder\n","    x = Conv3D(64, kernel, padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","    pool_1 = MaxPooling3D(pool_size=pool_size)(x)\n","    \n","    x = Conv3D(128, kernel, padding=\"same\")(pool_1)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","    pool_2 = MaxPooling3D(pool_size=pool_size)(x)\n","    \n","    x = Conv3D(256, kernel, padding=\"same\")(pool_2)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","    pool_3 = MaxPooling3D(pool_size=pool_size)(x)\n","    \n","    x = Conv3D(512,kernel, padding=\"same\")(pool_3)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","    \n","    # Decoder\n","    x = Conv3D(512, kernel, padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","    \n","    x = UpSampling3D(size=pool_size)(x)\n","    x = Conv3D(256, kernel, padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","    \n","    x = UpSampling3D(size=pool_size)(x)\n","    x = Conv3D(128, kernel, padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","    \n","    x = UpSampling3D(size=pool_size)(x)\n","    x = Conv3D(64, kernel, padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","    \n","    x = Conv3D(classes, 1, 1, padding=\"valid\")(x)\n","    #x = Reshape((input_shape[0]*input_shape[1]*input_shape[2], classes))(x)\n","    x = Activation(\"softmax\")(x)\n","    model = Model(img_input, x)\n","\n","\n","    return model\n","\n","\n","\n","model = SegNet(input_shape=(128,128,128,3), classes=4)\n","\n","model.summary()\n","print(model.input_shape)\n","print(model.output_shape)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"09pwQrpYTRF9","executionInfo":{"status":"ok","timestamp":1631203473292,"user_tz":-120,"elapsed":545,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"source":["import numpy as np\n","import nibabel as nib\n","import glob\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","from tifffile import imsave"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"-T4oryyxTfId"},"source":["from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","TRAIN_DATASET_PATH = '/content/drive/MyDrive/TrainingData/'\n","####### Custom Data Generation ####################\n","\n","import os\n","import numpy as np\n","\n","\n","def load_img(img_dir, img_list):\n","    images=[]\n","    for i, image_name in enumerate(img_list):    \n","        if (image_name.split('.')[1] == 'npy'):\n","            \n","            image = np.load(img_dir+image_name)\n","                      \n","            images.append(image)\n","    images = np.array(images)\n","    \n","    return(images)\n","\n","\n","\n","\n","def imageLoader(img_dir, img_list, mask_dir, mask_list, batch_size):\n","\n","    L = len(img_list)\n","\n","    #keras needs the generator infinite, so we will use while true  \n","    while True:\n","\n","        batch_start = 0\n","        batch_end = batch_size\n","\n","        while batch_start < L:\n","            limit = min(batch_end, L)\n","                       \n","            X = load_img(img_dir, img_list[batch_start:limit])\n","            Y = load_img(mask_dir, mask_list[batch_start:limit])\n","\n","            yield (X,Y) #a tuple with two numpy arrays with batch_size samples     \n","\n","            batch_start += batch_size   \n","            batch_end += batch_size\n","\n","############################################\n","\n","#Test the generator\n","\n","from matplotlib import pyplot as plt\n","import random\n","\n","train_img_dir = \"/content/drive/MyDrive/input_data_validation/train/images/\"\n","train_mask_dir = \"/content/drive/MyDrive/input_data_validation/train/masks/\"\n","train_img_list=os.listdir(train_img_dir)\n","train_mask_list = os.listdir(train_mask_dir)\n","\n","batch_size = 2\n","\n","train_img_datagen = imageLoader(train_img_dir, train_img_list, \n","                                train_mask_dir, train_mask_list, batch_size)\n","\n","#Verify generator.... In python 3 next() is renamed as __next__()\n","img, msk = train_img_datagen.__next__()\n","\n","\n","img_num = random.randint(0,img.shape[0]-1)\n","test_img=img[img_num]\n","test_mask=msk[img_num]\n","test_mask=np.argmax(test_mask, axis=3)\n","\n","n_slice=random.randint(0, test_mask.shape[2])\n","plt.figure(figsize=(12, 8))\n","\n","plt.subplot(221)\n","plt.imshow(test_img[:,:,n_slice, 0], cmap='gray')\n","plt.title('Image flair')\n","plt.subplot(222)\n","plt.imshow(test_img[:,:,n_slice, 1], cmap='gray')\n","plt.title('Image t1ce')\n","plt.subplot(223)\n","plt.imshow(test_img[:,:,n_slice, 2], cmap='gray')\n","plt.title('Image t2')\n","plt.subplot(224)\n","plt.imshow(test_mask[:,:,n_slice])\n","plt.title('Mask')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IvQtHc41TxjG","executionInfo":{"status":"ok","timestamp":1631203567228,"user_tz":-120,"elapsed":878,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"source":["train_img_dir = \"/content/drive/MyDrive/input_data_validation/train/images/\"\n","train_mask_dir = \"/content/drive/MyDrive/input_data_validation/train/masks/\"\n","\n","val_img_dir = \"/content/drive/MyDrive/input_data_validation/val/images/\"\n","val_mask_dir = \"/content/drive/MyDrive/input_data_validation/val/masks/\"\n","\n","train_img_list=os.listdir(train_img_dir)\n","train_mask_list = os.listdir(train_mask_dir)\n","\n","val_img_list=os.listdir(val_img_dir)\n","val_mask_list = os.listdir(val_mask_dir)\n","##################################\n","\n","########################################################################\n","batch_size = 1\n","\n","train_img_datagen = imageLoader(train_img_dir, train_img_list, \n","                                train_mask_dir, train_mask_list, batch_size)\n","\n","val_img_datagen = imageLoader(val_img_dir, val_img_list, \n","                                val_mask_dir, val_mask_list, batch_size)\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JueRFTNvT11b","executionInfo":{"status":"ok","timestamp":1631203604974,"user_tz":-120,"elapsed":13588,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"0c0ec6f6-09ef-47aa-b678-81ffe3e5dca9"},"source":["!pip install keras_applications\n","!pip install classification-models-3D\n","!pip install efficientnet-3D\n","!pip install segmentation-models-3D"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras_applications\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[?25l\r\u001b[K     |██████▌                         | 10 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 30 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 40 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 50 kB 3.1 MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications) (3.1.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications) (1.19.5)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications) (1.5.2)\n","Installing collected packages: keras-applications\n","Successfully installed keras-applications-1.0.8\n","Collecting classification-models-3D\n","  Downloading classification_models_3D-1.0.2-py3-none-any.whl (45 kB)\n","\u001b[K     |████████████████████████████████| 45 kB 1.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from classification-models-3D) (1.19.5)\n","Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from classification-models-3D) (2.6.0)\n","Installing collected packages: classification-models-3D\n","Successfully installed classification-models-3D-1.0.2\n","Collecting efficientnet-3D\n","  Downloading efficientnet_3D-1.0.1-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from efficientnet-3D) (1.19.5)\n","Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from efficientnet-3D) (2.6.0)\n","Installing collected packages: efficientnet-3D\n","Successfully installed efficientnet-3D-1.0.1\n","Collecting segmentation-models-3D\n","  Downloading segmentation_models_3D-1.0.1-py3-none-any.whl (32 kB)\n","Requirement already satisfied: classification-models-3D in /usr/local/lib/python3.7/dist-packages (from segmentation-models-3D) (1.0.2)\n","Requirement already satisfied: efficientnet-3D in /usr/local/lib/python3.7/dist-packages (from segmentation-models-3D) (1.0.1)\n","Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-3D) (2.6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from classification-models-3D->segmentation-models-3D) (1.19.5)\n","Installing collected packages: segmentation-models-3D\n","Successfully installed segmentation-models-3D-1.0.1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Robsd-FNT9_j","executionInfo":{"status":"ok","timestamp":1631203626307,"user_tz":-120,"elapsed":458,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"410c94ab-4ad0-4129-b3b8-51c2a4866a97"},"source":["import segmentation_models_3D as sm\n","wt0, wt1, wt2, wt3 = 0.25,0.25,0.25,0.25\n","#import segmentation_models_3D as sm\n","dice_loss = sm.losses.DiceLoss(class_weights=np.array([wt0, wt1, wt2, wt3])) \n","focal_loss = sm.losses.CategoricalFocalLoss()\n","total_loss = dice_loss + (1 * focal_loss)\n","\n","metrics = ['accuracy', sm.metrics.IOUScore(threshold=0.5)]\n","\n","LR = 0.0001\n","\n","from tensorflow.keras.optimizers import Adam\n","\n","#optim = tensorflow.keras.optimizers.Adam(LR)\n","\n","optim = Adam(LR)\n","#######################################################################\n","#Fit the model \n","\n","steps_per_epoch = len(train_img_list)//batch_size\n","val_steps_per_epoch = len(val_img_list)//batch_size\n","val_steps_per_epoch"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Segmentation Models: using `keras` framework.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-A3iIyYYevXj","executionInfo":{"status":"ok","timestamp":1631206440916,"user_tz":-120,"elapsed":176,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"122f1c9d-2264-41ea-8819-a08ae91d64c2"},"source":["val_steps_per_epoch"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILmfXNCNVoU9","executionInfo":{"status":"ok","timestamp":1631205279016,"user_tz":-120,"elapsed":185,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"115484e0-a011-4a58-8150-d663d9636277"},"source":["#gpu_options = tf.GPUOptions(allow_growth=True)\n","##session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n","tf.compat.v1.GPUOptions(allow_growth=True)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["allow_growth: true"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"PBCu2WVieupI"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":510},"id":"Ck_IwpTBUGML","executionInfo":{"status":"error","timestamp":1631206914421,"user_tz":-120,"elapsed":23685,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"3781e0bc-4ba5-4100-83eb-7d53e31ee87b"},"source":["model = SegNet(input_shape=(128,128,128,3), classes=4)\n","\n","model.compile(optimizer = optim, loss=total_loss, metrics=metrics)\n","#print(model.summary())\n","\n","#print(model.input_shape)\n","#print(model.output_shape)\n","\n","history=model.fit(train_img_datagen,\n","          epochs=30,\n","          validation_data=val_img_datagen,\n","          validation_steps=2,\n","          )"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n"]},{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-2f9f2e4b5c46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_img_datagen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m           )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[2,64,128,16384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_6/batch_normalization_55/FusedBatchNormV3 (defined at <ipython-input-18-2f9f2e4b5c46>:12) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_43468]\n\nFunction call stack:\ntrain_function\n"]}]}]}