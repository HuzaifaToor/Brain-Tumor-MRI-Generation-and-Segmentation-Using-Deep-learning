{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "progan_modules.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y98HmFxatGYM"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "class EqualLR:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def compute_weight(self, module):\n",
        "        weight = getattr(module, self.name + '_orig')\n",
        "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\n",
        "\n",
        "        return weight * sqrt(2 / fan_in)\n",
        "\n",
        "    @staticmethod\n",
        "    def apply(module, name):\n",
        "        fn = EqualLR(name)\n",
        "\n",
        "        weight = getattr(module, name)\n",
        "        del module._parameters[name]\n",
        "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
        "        module.register_forward_pre_hook(fn)\n",
        "\n",
        "        return fn\n",
        "\n",
        "    def __call__(self, module, input):\n",
        "        weight = self.compute_weight(module)\n",
        "        setattr(module, self.name, weight)\n",
        "\n",
        "\n",
        "def equal_lr(module, name='weight'):\n",
        "    EqualLR.apply(module, name)\n",
        "\n",
        "    return module\n",
        "\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input / torch.sqrt(torch.mean(input ** 2, dim=1, keepdim=True)\n",
        "                                  + 1e-8)\n",
        "\n",
        "\n",
        "class EqualConv2d(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        conv = nn.Conv2d(*args, **kwargs)\n",
        "        conv.weight.data.normal_()\n",
        "        conv.bias.data.zero_()\n",
        "        self.conv = equal_lr(conv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.conv(input)\n",
        "\n",
        "\n",
        "class EqualConvTranspose2d(nn.Module):\n",
        "    ### additional module for OOGAN usage\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        conv = nn.ConvTranspose2d(*args, **kwargs)\n",
        "        conv.weight.data.normal_()\n",
        "        conv.bias.data.zero_()\n",
        "        self.conv = equal_lr(conv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.conv(input)\n",
        "\n",
        "class EqualLinear(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        linear = nn.Linear(in_dim, out_dim)\n",
        "        linear.weight.data.normal_()\n",
        "        linear.bias.data.zero_()\n",
        "\n",
        "        self.linear = equal_lr(linear)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.linear(input)\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, padding, kernel_size2=None, padding2=None, pixel_norm=True):\n",
        "        super().__init__()\n",
        "\n",
        "        pad1 = padding\n",
        "        pad2 = padding\n",
        "        if padding2 is not None:\n",
        "            pad2 = padding2\n",
        "\n",
        "        kernel1 = kernel_size\n",
        "        kernel2 = kernel_size\n",
        "        if kernel_size2 is not None:\n",
        "            kernel2 = kernel_size2\n",
        "\n",
        "        convs = [EqualConv2d(in_channel, out_channel, kernel1, padding=pad1)]\n",
        "        if pixel_norm:\n",
        "            convs.append(PixelNorm())\n",
        "        convs.append(nn.LeakyReLU(0.1))\n",
        "        convs.append(EqualConv2d(out_channel, out_channel, kernel2, padding=pad2))\n",
        "        if pixel_norm:\n",
        "            convs.append(PixelNorm())\n",
        "        convs.append(nn.LeakyReLU(0.1))\n",
        "\n",
        "        self.conv = nn.Sequential(*convs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.conv(input)\n",
        "        return out\n",
        "\n",
        "\n",
        "def upscale(feat):\n",
        "    return F.interpolate(feat, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_code_dim=128, in_channel=128, pixel_norm=True, tanh=True):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_code_dim\n",
        "        self.tanh = tanh\n",
        "        self.input_layer = nn.Sequential(\n",
        "            EqualConvTranspose2d(input_code_dim, in_channel, 4, 1, 0),\n",
        "            PixelNorm(),\n",
        "            nn.LeakyReLU(0.1))\n",
        "\n",
        "        self.progression_4 = ConvBlock(in_channel, in_channel, 3, 1, pixel_norm=pixel_norm)\n",
        "        self.progression_8 = ConvBlock(in_channel, in_channel, 3, 1, pixel_norm=pixel_norm)\n",
        "        self.progression_16 = ConvBlock(in_channel, in_channel, 3, 1, pixel_norm=pixel_norm)\n",
        "        self.progression_32 = ConvBlock(in_channel, in_channel, 3, 1, pixel_norm=pixel_norm)\n",
        "        self.progression_64 = ConvBlock(in_channel, in_channel//2, 3, 1, pixel_norm=pixel_norm)\n",
        "        self.progression_128 = ConvBlock(in_channel//2, in_channel//4, 3, 1, pixel_norm=pixel_norm)\n",
        "        self.progression_256 = ConvBlock(in_channel//4, in_channel//4, 3, 1, pixel_norm=pixel_norm)\n",
        "\n",
        "        self.to_rgb_8 = EqualConv2d(in_channel, 3, 1)\n",
        "        self.to_rgb_16 = EqualConv2d(in_channel, 3, 1)\n",
        "        self.to_rgb_32 = EqualConv2d(in_channel, 3, 1)\n",
        "        self.to_rgb_64 = EqualConv2d(in_channel//2, 3, 1)\n",
        "        self.to_rgb_128 = EqualConv2d(in_channel//4, 3, 1)\n",
        "        self.to_rgb_256 = EqualConv2d(in_channel//4, 3, 1)\n",
        "        \n",
        "        self.max_step = 6\n",
        "\n",
        "    def progress(self, feat, module):\n",
        "        out = F.interpolate(feat, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        out = module(out)\n",
        "        return out\n",
        "\n",
        "    def output(self, feat1, feat2, module1, module2, alpha):\n",
        "        if 0 <= alpha < 1:\n",
        "            skip_rgb = upscale(module1(feat1))\n",
        "            out = (1-alpha)*skip_rgb + alpha*module2(feat2)\n",
        "        else:\n",
        "            out = module2(feat2)\n",
        "        if self.tanh:\n",
        "            return torch.tanh(out)\n",
        "        return out\n",
        "\n",
        "    def forward(self, input, step=0, alpha=-1):\n",
        "        if step > self.max_step:\n",
        "            step = self.max_step\n",
        "\n",
        "        out_4 = self.input_layer(input.view(-1, self.input_dim, 1, 1))\n",
        "        out_4 = self.progression_4(out_4)\n",
        "        out_8 = self.progress(out_4, self.progression_8)\n",
        "        if step==1:\n",
        "            if self.tanh:\n",
        "                return torch.tanh(self.to_rgb_8(out_8))\n",
        "            return self.to_rgb_8(out_8)\n",
        "        \n",
        "        out_16 = self.progress(out_8, self.progression_16)\n",
        "        if step==2:\n",
        "            return self.output( out_8, out_16, self.to_rgb_8, self.to_rgb_16, alpha )\n",
        "        \n",
        "        out_32 = self.progress(out_16, self.progression_32)\n",
        "        if step==3:\n",
        "            return self.output( out_16, out_32, self.to_rgb_16, self.to_rgb_32, alpha )\n",
        "\n",
        "        out_64 = self.progress(out_32, self.progression_64)\n",
        "        if step==4:\n",
        "            return self.output( out_32, out_64, self.to_rgb_32, self.to_rgb_64, alpha )\n",
        "        \n",
        "        out_128 = self.progress(out_64, self.progression_128)\n",
        "        if step==5:\n",
        "            return self.output( out_64, out_128, self.to_rgb_64, self.to_rgb_128, alpha )\n",
        "\n",
        "        out_256 = self.progress(out_128, self.progression_256)\n",
        "        if step==6:\n",
        "            return self.output( out_128, out_256, self.to_rgb_128, self.to_rgb_256, alpha )\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, feat_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.progression = nn.ModuleList([ConvBlock(feat_dim//4, feat_dim//4, 3, 1),\n",
        "                                          ConvBlock(feat_dim//4, feat_dim//2, 3, 1),\n",
        "                                          ConvBlock(feat_dim//2, feat_dim, 3, 1),\n",
        "                                          ConvBlock(feat_dim, feat_dim, 3, 1),\n",
        "                                          ConvBlock(feat_dim, feat_dim, 3, 1),\n",
        "                                          ConvBlock(feat_dim, feat_dim, 3, 1),\n",
        "                                          ConvBlock(feat_dim+1, feat_dim, 3, 1, 4, 0)])\n",
        "\n",
        "        self.from_rgb = nn.ModuleList([EqualConv2d(3, feat_dim//4, 1),\n",
        "                                       EqualConv2d(3, feat_dim//4, 1),\n",
        "                                       EqualConv2d(3, feat_dim//2, 1),\n",
        "                                       EqualConv2d(3, feat_dim, 1),\n",
        "                                       EqualConv2d(3, feat_dim, 1),\n",
        "                                       EqualConv2d(3, feat_dim, 1),\n",
        "                                       EqualConv2d(3, feat_dim, 1)])\n",
        "\n",
        "        self.n_layer = len(self.progression)\n",
        "\n",
        "        self.linear = EqualLinear(feat_dim, 1)\n",
        "\n",
        "    def forward(self, input, step=0, alpha=-1):\n",
        "        for i in range(step, -1, -1):\n",
        "            index = self.n_layer - i - 1\n",
        "\n",
        "            if i == step:\n",
        "                out = self.from_rgb[index](input)\n",
        "\n",
        "            if i == 0:\n",
        "                out_std = torch.sqrt(out.var(0, unbiased=False) + 1e-8)\n",
        "                mean_std = out_std.mean()\n",
        "                mean_std = mean_std.expand(out.size(0), 1, 4, 4)\n",
        "                out = torch.cat([out, mean_std], 1)\n",
        "\n",
        "            out = self.progression[index](out)\n",
        "\n",
        "            if i > 0:\n",
        "                # out = F.avg_pool2d(out, 2)\n",
        "                out = F.interpolate(out, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "\n",
        "                if i == step and 0 <= alpha < 1:\n",
        "                    # skip_rgb = F.avg_pool2d(input, 2)\n",
        "                    skip_rgb = F.interpolate(input, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "                    skip_rgb = self.from_rgb[index + 1](skip_rgb)\n",
        "                    out = (1 - alpha) * skip_rgb + alpha * out\n",
        "\n",
        "        out = out.squeeze(2).squeeze(2)\n",
        "        # print(input.size(), out.size(), step)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY59JHqAtMA9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}