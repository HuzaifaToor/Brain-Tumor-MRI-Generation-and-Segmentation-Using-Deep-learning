{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SegNet15.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1iA9bqukgDkNAXLidOMbkj4VXH45qHh97","authorship_tag":"ABX9TyP8MX03q3+PqjcOuhQ5Tj46"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"dh9TR1SGI0vK","executionInfo":{"status":"ok","timestamp":1634039286638,"user_tz":-120,"elapsed":1487,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from glob import glob\n","import pickle\n","import tensorflow as tf\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"SW5KL7NPZzc2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKy3uisKJDEL","executionInfo":{"status":"ok","timestamp":1634039291748,"user_tz":-120,"elapsed":4176,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"source":["x_train = np.load('/content/drive/MyDrive/brain tumor segmentation/huzaifa data/Prepared_MRI_data/Origna_data_handling/Stack_Arrays/train_images.npy')\n","x_test = np.load('/content/drive/MyDrive/brain tumor segmentation/huzaifa data/Prepared_MRI_data/Origna_data_handling/Stack_Arrays/val_images.npy')\n","y_train = np.load('/content/drive/MyDrive/brain tumor segmentation/huzaifa data/Prepared_MRI_data/Origna_data_handling/Stack_Arrays/train_masks.npy')\n","y_test = np.load('/content/drive/MyDrive/brain tumor segmentation/huzaifa data/Prepared_MRI_data/Origna_data_handling/Stack_Arrays/val_masks.npy')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"cMbNb8pxZ0Ti","executionInfo":{"status":"ok","timestamp":1634039315793,"user_tz":-120,"elapsed":12836,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"source":["from torch.nn import MaxUnpool2d as MaxUnpooling2D"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"OxhYn4L_KO2p","executionInfo":{"status":"ok","timestamp":1634039294725,"user_tz":-120,"elapsed":416,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"source":["from keras import backend as K\n","from keras.layers import Layer\n","import tensorflow as tf\n","\n","\n","class MaxPoolingWithArgmax2D(Layer):\n","\n","    def __init__(\n","            self,\n","            pool_size=(2, 2),\n","            strides=(2, 2),\n","            padding='same',\n","            **kwargs):\n","        super(MaxPoolingWithArgmax2D, self).__init__(**kwargs)\n","        self.padding = padding\n","        self.pool_size = pool_size\n","        self.strides = strides\n","\n","    def call(self, inputs, **kwargs):\n","        print(\"max pooling with argmax\")\n","        padding = self.padding\n","        pool_size = self.pool_size\n","        strides = self.strides\n","        if K.backend() == 'tensorflow':\n","            ksize = [1, pool_size[0], pool_size[1], 1]\n","            padding = padding.upper()\n","            strides = [1, strides[0], strides[1], 1]\n","            output, argmax = tf.nn.max_pool_with_argmax(\n","                    inputs,\n","                    ksize=ksize,\n","                    strides=strides,\n","                    padding=padding)\n","        else:\n","            errmsg = '{} backend is not supported for layer {}'.format(\n","                    K.backend(), type(self).__name__)\n","            raise NotImplementedError(errmsg)\n","        argmax = K.cast(argmax, K.floatx())\n","        return [output, argmax]\n","\n","    def compute_output_shape(self, input_shape):\n","        print(\"i guess its subsampling\")\n","        ratio = (1, 2, 2, 1)\n","        output_shape = [\n","                dim//ratio[idx]\n","                if dim is not None else None\n","                for idx, dim in enumerate(input_shape)]\n","        output_shape = tuple(output_shape)\n","        return [output_shape, output_shape]\n","\n","\n","    def compute_mask(self, inputs, mask=None):\n","        print(\"no idea what this is: but computing mask\")\n","        return 2 * [None]\n","\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-7xULHBT5OT"},"source":["'''class MaxUnpooling2D(Layer):\n","    def __init__(self, size=(2, 2), **kwargs):\n","        super(MaxUnpooling2D, self).__init__(**kwargs)\n","        self.size = size\n","        batch_size = size\n","    def call(self, inputs, output_shape=None):\n","        # one is pool and one is mask\n","        updates, mask = inputs[0], inputs[1]\n","        pool = updates\n","        batch_size = self.size\n","        with tf.compat.v1.variable_scope(self.name):\n","            pool_ = tf.reshape(pool, [-1])\n","            batch_range = tf.reshape(tf.range(batch_size, dtype=ind.dtype), [tf.shape(pool)[0], 1, 1, 1])\n","            b = tf.ones_like(ind) * batch_range\n","            b = tf.reshape(b, [-1, 1])\n","            ind_ = tf.reshape(ind, [-1, 1])\n","            ind_ = tf.concat([b, ind_], 1)\n","            ret = tf.scatter_nd(ind_, pool_, shape=[batch_size, output_shape[1] * output_shape[2] * output_shape[3]])\n","            # the reason that we use tf.scatter_nd: if we use tf.sparse_tensor_to_dense, then the gradient is None, which will cut off the network.\n","            # But if we use tf.scatter_nd, the gradients for all the trainable variables will be tensors, instead of None.\n","            # The usage for tf.scatter_nd is that: create a new tensor by applying sparse UPDATES(which is the pooling value) to individual values of slices within a\n","            # zero tensor of given shape (FLAT_OUTPUT_SHAPE) according to the indices (ind_). If we ues the orignal code, the only thing we need to change is: changeing\n","            # from tf.sparse_tensor_to_dense(sparse_tensor) to tf.sparse_add(tf.zeros((output_sahpe)),sparse_tensor) which will give us the gradients!!!\n","            ret = tf.reshape(ret, [tf.shape(pool)[0], output_shape[1], output_shape[2], output_shape[3]])\n","            return ret\n","\n","    def compute_output_shape(self, input_shape):\n","        mask_shape = input_shape[1]\n","        return (\n","                mask_shape[0],\n","                mask_shape[1]*self.size[0],\n","                mask_shape[2]*self.size[1],\n","                mask_shape[3]\n","                )'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kmcU9cNhTz8S"},"source":["'''def MaxUnpooling2D(pool, ind, output_shape, batch_size, name=None):\n","\"\"\"\n","   Unpooling layer after max_pool_with_argmax.\n","   Args:\n","       pool:   max pooled output tensor\n","       ind:      argmax indices\n","       ksize:     ksize is the same as for the pool\n","   Return:\n","       unpool:    unpooling tensor\n","       :param batch_size:\n","\"\"\"\n","with tf.compat.v1.variable_scope(name):\n","    pool_ = tf.reshape(pool, [-1])\n","    batch_range = tf.reshape(tf.range(batch_size, dtype=ind.dtype), [tf.shape(pool)[0], 1, 1, 1])\n","    b = tf.ones_like(ind) * batch_range\n","    b = tf.reshape(b, [-1, 1])\n","    ind_ = tf.reshape(ind, [-1, 1])\n","    ind_ = tf.concat([b, ind_], 1)\n","    ret = tf.scatter_nd(ind_, pool_, shape=[batch_size, output_shape[1] * output_shape[2] * output_shape[3]])\n","    # the reason that we use tf.scatter_nd: if we use tf.sparse_tensor_to_dense, then the gradient is None, which will cut off the network.\n","    # But if we use tf.scatter_nd, the gradients for all the trainable variables will be tensors, instead of None.\n","    # The usage for tf.scatter_nd is that: create a new tensor by applying sparse UPDATES(which is the pooling value) to individual values of slices within a\n","    # zero tensor of given shape (FLAT_OUTPUT_SHAPE) according to the indices (ind_). If we ues the orignal code, the only thing we need to change is: changeing\n","    # from tf.sparse_tensor_to_dense(sparse_tensor) to tf.sparse_add(tf.zeros((output_sahpe)),sparse_tensor) which will give us the gradients!!!\n","    ret = tf.reshape(ret, [tf.shape(pool)[0], output_shape[1], output_shape[2], output_shape[3]])\n","    return ret'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":544},"id":"ci1JVvwlLPuP","executionInfo":{"status":"error","timestamp":1634039403444,"user_tz":-120,"elapsed":546,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"c3d83d72-eb10-4d01-9c64-f477e29929db"},"source":["from keras.models import Model\n","from keras.layers import Input\n","from keras.layers.core import Activation, Reshape, Dense\n","from keras.layers.convolutional import Convolution2D\n","from keras.layers import BatchNormalization\n","\n","#from layers import MaxPoolingWithArgmax2D, MaxUnpooling2D\n","\n","def segnet(\n","        input_shape,\n","        n_labels,\n","        kernel=3,\n","        pool_size=(2, 2),\n","        output_mode=\"softmax\"):\n","    # encoder\n","    inputs = Input(shape=input_shape)\n","\n","    conv_1 = Convolution2D(64, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal', name=\"block1_conv1\")(inputs)\n","    conv_1 = BatchNormalization()(conv_1)\n","    conv_1 = Activation(\"relu\")(conv_1)\n","    conv_2 = Convolution2D(64, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal', name=\"block1_conv2\")(conv_1)\n","    conv_2 = BatchNormalization()(conv_2)\n","    conv_2 = Activation(\"relu\")(conv_2)\n","    pool_1, mask_1 = MaxPoolingWithArgmax2D(pool_size, name=\"block1_pool\")(conv_2)\n","\n","    conv_3 = Convolution2D(128, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal', name=\"block2_conv1\")(pool_1)\n","    conv_3 = BatchNormalization()(conv_3)\n","    conv_3 = Activation(\"relu\")(conv_3)\n","    conv_4 = Convolution2D(128, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal',  name=\"block2_conv2\")(conv_3)\n","    conv_4 = BatchNormalization()(conv_4)\n","    conv_4 = Activation(\"relu\")(conv_4)\n","\n","    pool_2, mask_2 = MaxPoolingWithArgmax2D(pool_size, name=\"block2_pool\")(conv_4)\n","\n","    conv_5 = Convolution2D(256, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal', name=\"block3_conv1\")(pool_2)\n","    conv_5 = BatchNormalization()(conv_5)\n","    conv_5 = Activation(\"relu\")(conv_5)\n","    conv_6 = Convolution2D(256, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal', name=\"block3_conv2\")(conv_5)\n","    conv_6 = BatchNormalization()(conv_6)\n","    conv_6 = Activation(\"relu\")(conv_6)\n","    conv_7 = Convolution2D(256, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal', name=\"block3_conv3\")(conv_6)\n","    conv_7 = BatchNormalization()(conv_7)\n","    conv_7 = Activation(\"relu\")(conv_7)\n","\n","    pool_3, mask_3 = MaxPoolingWithArgmax2D(pool_size, name=\"block3_pool\")(conv_7)\n","\n","    conv_8 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal', name=\"block4_conv1\")(pool_3)\n","    conv_8 = BatchNormalization()(conv_8)\n","    conv_8 = Activation(\"relu\")(conv_8)\n","    conv_9 = Convolution2D(512, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal', name=\"block4_conv2\")(conv_8)\n","    conv_9 = BatchNormalization()(conv_9)\n","    conv_9 = Activation(\"relu\")(conv_9)\n","    conv_10 = Convolution2D(512, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal', name=\"block4_conv3\")(conv_9)\n","    conv_10 = BatchNormalization()(conv_10)\n","    conv_10 = Activation(\"relu\")(conv_10)\n","\n","    pool_4, mask_4 = MaxPoolingWithArgmax2D(pool_size, name=\"block4_pool\")(conv_10)\n","\n","    conv_11 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal', name=\"block5_conv1\")(pool_4)\n","    conv_11 = BatchNormalization()(conv_11)\n","    conv_11 = Activation(\"relu\")(conv_11)\n","    conv_12 = Convolution2D(512, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal',  name=\"block5_conv2\")(conv_11)\n","    conv_12 = BatchNormalization()(conv_12)\n","    conv_12 = Activation(\"relu\")(conv_12)\n","    conv_13 = Convolution2D(512, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal',  name=\"block5_conv3\")(conv_12)\n","    conv_13 = BatchNormalization()(conv_13)\n","    conv_13 = Activation(\"relu\")(conv_13)\n","\n","    pool_5, mask_5 = MaxPoolingWithArgmax2D(pool_size, name=\"block5_pool\")(conv_13)\n","   \n","    # decoder\n","    unpool_1 = MaxUnpooling2D(pool_size, pool_5)([pool_5, mask_5])\n","\n","    conv_14 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(unpool_1)\n","    conv_14 = BatchNormalization()(conv_14)\n","    conv_14 = Activation(\"relu\")(conv_14)\n","    conv_15 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_14)\n","    conv_15 = BatchNormalization()(conv_15)\n","    conv_15 = Activation(\"relu\")(conv_15)\n","    conv_16 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_15)\n","    conv_16 = BatchNormalization()(conv_16)\n","    conv_16 = Activation(\"relu\")(conv_16)\n","\n","    unpool_2 = MaxUnpooling2D(pool_size)([conv_16, mask_4])\n","\n","    conv_17 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(unpool_2)\n","    conv_17 = BatchNormalization()(conv_17)\n","    conv_17 = Activation(\"relu\")(conv_17)\n","    conv_18 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_17)\n","    conv_18 = BatchNormalization()(conv_18)\n","    conv_18 = Activation(\"relu\")(conv_18)\n","    conv_19 = Convolution2D(256, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_18)\n","    conv_19 = BatchNormalization()(conv_19)\n","    conv_19 = Activation(\"relu\")(conv_19)\n","\n","    unpool_3 = MaxUnpooling2D(pool_size)([conv_19, mask_3])\n","\n","    conv_20 = Convolution2D(256, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(unpool_3)\n","    conv_20 = BatchNormalization()(conv_20)\n","    conv_20 = Activation(\"relu\")(conv_20)\n","    conv_21 = Convolution2D(256, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_20)\n","    conv_21 = BatchNormalization()(conv_21)\n","    conv_21 = Activation(\"relu\")(conv_21)\n","    conv_22 = Convolution2D(128, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_21)\n","    conv_22 = BatchNormalization()(conv_22)\n","    conv_22 = Activation(\"relu\")(conv_22)\n","\n","    unpool_4 = MaxUnpooling2D(pool_size)([conv_22, mask_2])\n","\n","    conv_23 = Convolution2D(128, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(unpool_4)\n","    conv_23 = BatchNormalization()(conv_23)\n","    conv_23 = Activation(\"relu\")(conv_23)\n","    conv_24 = Convolution2D(64, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_23)\n","    conv_24 = BatchNormalization()(conv_24)\n","    conv_24 = Activation(\"relu\")(conv_24)\n","\n","    unpool_5 = MaxUnpooling2D(pool_size)([conv_24, mask_1])\n","    #pool, ind, output_shape, batch_size, name=None\n","\n","    conv_25 = Convolution2D(64, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(unpool_5)\n","    conv_25 = BatchNormalization()(conv_25)\n","    conv_25 = Activation(\"relu\")(conv_25)\n","\n","    conv_26 = Convolution2D(n_labels, (1, 1), padding=\"same\", kernel_initializer='he_normal')(conv_25)\n","    conv_26 = BatchNormalization()(conv_26)\n","\n","    conv_26 = Dense(12)(conv_26)\n","    outputs = Activation(output_mode)(conv_26)\n","    print(\"Build decoder done..\")\n","\n","    model = Model(inputs=inputs, outputs=outputs, name=\"SegNet\")\n","\n","    return model\n","\n","\n","model = segnet(input_shape=(128,128,128), n_labels=4)\n","\n","model.summary()\n","print(model.input_shape)\n","print(model.output_shape)\n","print(model.input_shape)\n","print(model.output_shape)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["max pooling with argmax\n","no idea what this is: but computing mask\n","max pooling with argmax\n","no idea what this is: but computing mask\n","max pooling with argmax\n","no idea what this is: but computing mask\n","max pooling with argmax\n","no idea what this is: but computing mask\n","max pooling with argmax\n","no idea what this is: but computing mask\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-9850687b0a1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-9850687b0a1a>\u001b[0m in \u001b[0;36msegnet\u001b[0;34m(input_shape, n_labels, kernel, pool_size, output_mode)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0munpool_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxUnpooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpool_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mconv_14\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvolution2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'he_normal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpool_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, kernel_size, stride, padding)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxUnpool2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstride\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/utils.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/keras_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m       raise TypeError(\n\u001b[0;32m--> 343\u001b[0;31m           'Cannot iterate over a Tensor with unknown first dimension.')\n\u001b[0m\u001b[1;32m    344\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_KerasTensorIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Cannot iterate over a Tensor with unknown first dimension."]}]}]}