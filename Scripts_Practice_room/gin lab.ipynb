{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gin lab.ipynb","provenance":[],"authorship_tag":"ABX9TyNVJxxsenNJZ/nh2Vz7u2VG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XfgavU4fW5EI","executionInfo":{"status":"ok","timestamp":1638002931704,"user_tz":-60,"elapsed":2981,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"9f7afee7-af86-4ce0-fdd0-7b5767370b9b"},"source":["!git clone https://github.com/volotat/GANLib.git"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'GANLib'...\n","remote: Enumerating objects: 1699, done.\u001b[K\n","remote: Total 1699 (delta 0), reused 0 (delta 0), pack-reused 1699\u001b[K\n","Receiving objects: 100% (1699/1699), 40.34 MiB | 27.28 MiB/s, done.\n","Resolving deltas: 100% (968/968), done.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p38l-yI1W6ss","executionInfo":{"status":"ok","timestamp":1638002958015,"user_tz":-60,"elapsed":21399,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"45bba316-2a4b-4a9b-94b5-627d3959e79a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tswzR7sXIuH","executionInfo":{"status":"ok","timestamp":1638002958016,"user_tz":-60,"elapsed":8,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"1748fd07-1043-4e7f-9131-1ab01fe92c6f"},"source":["%ls "],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34mGANLib\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZOp8c-T1XVLh","executionInfo":{"status":"ok","timestamp":1638002961252,"user_tz":-60,"elapsed":216,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"28cc5dfe-2b35-4c1c-9f40-5c9de56089ba"},"source":["cd GANLib/"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/GANLib\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wcV2wSaXWrQ","executionInfo":{"status":"ok","timestamp":1638002968379,"user_tz":-60,"elapsed":231,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"48379601-40c7-4bba-eed2-afb3eb0077b0"},"source":["ls"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["100DMLCLog.md  \u001b[0m\u001b[01;34mexamples\u001b[0m/  \u001b[01;34mGANLib\u001b[0m/  README.md  setup.py  \u001b[01;34mtests\u001b[0m/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dOPtv2-VXW-M","executionInfo":{"status":"ok","timestamp":1638002971055,"user_tz":-60,"elapsed":210,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"a7188275-aee2-435a-dd78-185321662a48"},"source":["cd examples/"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/GANLib/examples\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6l4hRW3qXYZw","executionInfo":{"status":"ok","timestamp":1638002973933,"user_tz":-60,"elapsed":227,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"6ccc95ed-7fde-41d3-9903-2868241d135a"},"source":["ls"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["pg_gan_fast.py  pg_gan.py        simple_gan.py\n","pg_gan_.png     simple_gan_.png  training.gif\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IBtRkSeFXbhv","executionInfo":{"status":"ok","timestamp":1638003081946,"user_tz":-60,"elapsed":217,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"c918338f-4129-497f-d7d9-8d307c8a6e80"},"source":["!python3 pg_gan.py"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"pg_gan.py\", line 1, in <module>\n","    from GANLib import GAN, utils, distances\n","ModuleNotFoundError: No module named 'GANLib'\n"]}]},{"cell_type":"code","metadata":{"id":"jTHoADAuXk2O","executionInfo":{"status":"ok","timestamp":1638003068640,"user_tz":-60,"elapsed":229,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"source":["import sys\n","sys.path.append('/content/GANLib/GANLib/GANs')"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"HAemVMTmX43I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638003072180,"user_tz":-60,"elapsed":205,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"f7ef40f6-0d84-4c0d-d700-c4eb3457a98a"},"source":["ls"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["pg_gan_fast.py  pg_gan.py        simple_gan.py\n","pg_gan_.png     simple_gan_.png  training.gif\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qW9cYkS7l1on","executionInfo":{"status":"ok","timestamp":1638003451021,"user_tz":-60,"elapsed":244,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"a780f4b7-63a0-45e8-b8ab-32ad0d5e38bf"},"source":["!nano pg_gan.py"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: nano: command not found\n"]}]},{"cell_type":"code","metadata":{"id":"6JJPIF6HnSJM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AL1rvAN_nZYU"},"source":["**code starts from here**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wYhlR0bWnoNk","executionInfo":{"status":"ok","timestamp":1638003730108,"user_tz":-60,"elapsed":728,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"e032100d-51f6-4afa-8723-4c7cf4b5937b"},"source":["!cp/content/GANLib/GANLib/distances.py"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: cp/content/GANLib/GANLib/distances.py: No such file or directory\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"bLqYhcV9nhQx","executionInfo":{"status":"error","timestamp":1638003516527,"user_tz":-60,"elapsed":1226,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"d43de6e7-500a-4e24-a7b4-6d0624845725"},"source":["from GANLib import GAN, utils, distances\n","import \n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","\n","\n","#                   Progressive Growing of GANs\n","#   Paper: https://arxiv.org/pdf/1710.10196.pdf\n","\n","#       Description:\n","#   Takes as input some dataset and trains the network as usual GAN but progressively \n","#   adding layers to generator and discriminator.\n","\n","#-------------------------------\n","# Auxiliary functions\n","#-------------------------------  \n","\n","def augment(data):\n","    off_x_p = data.copy()\n","    off_x_p[:,1:,:,:] = off_x_p[:,:-1,:,:]\n","    off_x_m = data.copy()\n","    off_x_p[:,:-1,:,:] = off_x_p[:,1:,:,:]\n","    data = np.concatenate((data,off_x_p,off_x_m), axis = 0)\n","    \n","    off_y_p = data.copy()\n","    off_y_p[:,:,1:,:] = off_y_p[:,:,:-1,:]\n","    off_y_m = data.copy()\n","    off_y_m[:,:,:-1,:] = off_y_m[:,:,1:,:]\n","    data = np.concatenate((data,off_y_p,off_y_m), axis = 0)\n","    \n","    return data\n","    \n","def upscale2d(x, factor=2):\n","    assert isinstance(factor, int) and factor >= 1\n","    if factor == 1: return x\n","    with tf.variable_scope('Upscale2D'):\n","        s = x.shape\n","        x = tf.reshape(x, [-1, s[1], 1, s[2], 1, s[3]])\n","        x = tf.tile(x, [1, 1, factor, 1, factor, 1])\n","        x = tf.reshape(x, [-1, s[1] * factor, s[2] * factor, s[3]])\n","        return x\n","        \n","def get_scaled_weight(shape, dtype, partition_info):\n","    #He's normal dynamic weight scaler\n","    if len(shape) == 2:\n","        fan_in = shape[0]\n","    else:\n","        receptive_field_size = np.prod(shape[:-2])\n","        fan_in = shape[-2] * receptive_field_size\n","        \n","    std = np.sqrt(2 / max(1., fan_in))\n","    return tf.get_variable(\"w\", shape=shape, initializer=tf.initializers.random_normal(0, 1), dtype = tf.float32) * std\n"," \n","#-------------------------------\n","# Define models structure\n","#-------------------------------      \n","      \n","initialization = get_scaled_weight\n","\n","noise_dim = 64 \n","channels = 3\n","\n","weights = {}\n","sheets = 0\n","\n","\n","def new_sheet(filters, kernel_size, padding, name, pix_norm = True):\n","    def func(layer):\n","        layer = tf.layers.conv2d(layer, filters, kernel_size, padding=padding, name = name, kernel_initializer = initialization)\n","        layer = tf.nn.leaky_relu(layer, alpha=0.2) \n","        if pix_norm: layer = utils.PixelNorm(layer)\n","        return layer\n","    return func\n","    \n","def transition_alpha(gan):\n","    epoch = tf.cast(gan.epoch, tf.float32)\n","    epochs = tf.cast(gan.epochs, tf.float32)\n","    a = epoch / (epochs/ 2)\n","    b = 1\n","    return tf.minimum(a, b) \n","   \n","def generator(input, gan):\n","    previous_step = None\n","    next_step = None\n","\n","    layer = input\n","    \n","    layer = tf.keras.layers.RepeatVector(16)(layer)\n","    layer = tf.keras.layers.Reshape((4, 4, noise_dim))(layer)\n","    \n","    layer = new_sheet(filters_list[0], (4,4), 'same', 'genr_head_0')(layer)\n","    layer = new_sheet(filters_list[0], (3,3), 'same', 'genr_head_1')(layer)\n","    \n","    #Growing layers\n","    for i in range(sheets):\n","        s = image_size_list[i + 1]\n","        layer = upscale2d(layer)\n","        if i == sheets-1: previous_step = layer\n","            \n","        layer = new_sheet(filters_list[i+1], (3,3), 'same', 'genr_layer_a'+str(i))(layer)\n","        layer = new_sheet(filters_list[i+1], (3,3), 'same', 'genr_layer_b'+str(i))(layer)\n","   \n","    next_step = tf.layers.conv2d(layer, channels, (1,1), name = 'to_rgb_'+str(sheets), kernel_initializer = initialization) #to RGB\n","    \n","    #smooth fading\n","    if previous_step is not None: \n","        previous_step = tf.layers.conv2d(previous_step, channels, (1,1), name = 'to_rgb_'+str(sheets - 1)) \n","        layer = previous_step + (next_step - previous_step) * transition_alpha(gan)\n","    else:\n","        layer = next_step\n","      \n","    return layer\n","    \n","def discriminator(input, gan):\n","    previous_step = None\n","    next_step = None\n","    \n","    input_layer = input \n","    \n","    layer = tf.layers.conv2d(input, filters_list[sheets], (1,1), name = 'from_rgb_'+str(sheets), kernel_initializer = initialization) #from RGB\n","    layer = tf.nn.leaky_relu(layer, alpha=0.2)\n","    \n","    #Growing layers\n","    for i in range(sheets, 0, -1):\n","        layer = new_sheet(filters_list[i], (3,3), 'same', 'disc_layer_b'+str(i), pix_norm = False)(layer)\n","        layer = new_sheet(filters_list[i - 1], (3,3), 'same', 'disc_layer_a'+str(i), pix_norm = False)(layer)\n","        layer = tf.layers.average_pooling2d(layer, 2, 2)\n","\n","        #smooth fading\n","        if i == sheets:\n","            next_step = layer\n","            \n","            previous_step = tf.layers.average_pooling2d(input_layer, 2, 2)\n","            previous_step = tf.layers.conv2d(previous_step, filters_list[i - 1], (1,1), name = 'from_rgb_'+str(sheets - 1), kernel_initializer = initialization) #from RGB\n","            previous_step = tf.nn.leaky_relu(previous_step, alpha=0.2)\n","        \n","            layer = previous_step + (next_step - previous_step) * transition_alpha(gan)\n","                \n","    \n","    layer = utils.MiniBatchStddev(layer, group_size=4)\n","    layer = new_sheet(filters_list[0], (3,3), 'same', 'disc_head_0', pix_norm = False)(layer)\n","    layer = new_sheet(filters_list[0], (4,4), 'valid', 'disc_head_1', pix_norm = False)(layer)\n","    \n","    layer = tf.keras.layers.Flatten()(layer)\n","    layer = tf.layers.dense(layer, 1, kernel_initializer = initialization)\n","\n","    return layer\n","    \n","#-------------------------------\n","#  Main code\n","#-------------------------------  \n","\n","r, c = 3, 5\n","sample_noise = np.random.uniform(-1, 1, (r * c, noise_dim))\n","def sample_images(gen, file):\n","    gen_imgs = gen.predict(sample_noise, moving_avarage = True)\n","\n","    # Rescale images 0 - 1\n","    gen_imgs = 0.5 * gen_imgs + 0.5\n","    gen_imgs = np.clip(gen_imgs,0,1)\n","    \n","    fig, axs = plt.subplots(r, c)\n","    cnt = 0\n","    for i in range(r):\n","        for j in range(c):\n","            if gen_imgs.shape[-1] == 1: \n","                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n","            else:\n","                axs[i,j].imshow(gen_imgs[cnt,:,:])\n","            axs[i,j].axis('off')\n","            cnt += 1\n","    \n","    fig.savefig(file)\n","    plt.close()   \n","    \n","    \n","# Load the dataset\n","(dataseta, labelsa), (datasetb, labelsb) = tf.keras.datasets.cifar10.load_data()\n","dataset = np.concatenate((dataseta,datasetb), axis = 0)\n","labels = np.concatenate((labelsa,labelsb), axis = 0)\n","\n","indx = np.where(labels == 8)[0] # we choose only one specific domain from the dataset\n","dataset = dataset[indx]\n","\n","# Configure input\n","dataset = (dataset.astype(np.float32) - 127.5) / 127.5\n","if len(dataset.shape)<4:\n","    dataset = np.expand_dims(dataset, axis=3)\n","    \n","\n","# 6000 examples is not enough, so we augment dataset by shifting it along axis by 1 pixel \n","dataset = augment(dataset)\n"," \n","epochs_list = [4000, 8000, 16000, 32000]\n","batch_size_list = [16, 16, 16, 16]  \n","image_size_list = [4, 8, 16, 32] \n","filters_list = [48, 32, 24, 16]\n","\n","optimizer = tf.train.AdamOptimizer(0.001, 0., 0.99, epsilon = 1e-08) #Hyperparameters for optimizer from paper\n","with tf.Session() as sess:\n","    t = time.time()\n","    dataset_t = tf.Variable(np.zeros_like(dataset), dtype = tf.float32)\n","    for i in range(len(epochs_list)):    \n","        epochs = epochs_list[i]\n","        batch_size = batch_size_list[i]\n","        \n","        data_set = sess.run(tf.image.resize_bilinear(dataset_t, (image_size_list[i], image_size_list[i])), feed_dict = {dataset_t: dataset})\n","        print(data_set.shape)\n","        \n","        # Build and train GAN\n","        gan = GAN(sess, data_set.shape[1:], noise_dim, optimizer = optimizer, distance = distances.wasserstein_gp)\n","        gan.generator = lambda x: generator(x, gan) #define generator model\n","        gan.discriminator = lambda x: discriminator(x, gan) #define discriminator model\n","        \n","        def callback():\n","            sample_images(gan, 'pg_gan.png')\n","            \n","        gan.train(data_set, epochs = epochs, batch_size = batch_size, checkpoint_callback = callback, collect_history = False)  \n","        sheets += 1\n","        \n","    print('Training complete! Total traning time: %f s'%(time.time() - t))   \n"],"execution_count":13,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-fcbb25dfda27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mGANLib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'GAN' from 'GANLib' (unknown location)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"oS0Z7tSDnh5e"},"source":[""],"execution_count":null,"outputs":[]}]}