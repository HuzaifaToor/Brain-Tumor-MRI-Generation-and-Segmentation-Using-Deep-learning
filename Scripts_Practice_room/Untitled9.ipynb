{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled9.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMZIJKJBENFTstjznDIaKjm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"S3NM0raV_GMt","executionInfo":{"status":"ok","timestamp":1637591309092,"user_tz":-60,"elapsed":299,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"source":["from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten\n","from keras.layers import BatchNormalization\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten\n","from keras.layers import BatchNormalization\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.models import Sequential, Model\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMhYrrU5CqjR","executionInfo":{"status":"ok","timestamp":1637591211361,"user_tz":-60,"elapsed":254,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"source":["img_rows = 128\n","img_cols = 128\n","channels = 1\n","img_shape = (img_rows, img_cols, channels)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"1n1DDLN8Aags","executionInfo":{"status":"ok","timestamp":1637591162838,"user_tz":-60,"elapsed":251,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"source":["def build_generator():\n","\n","    noise_shape = (100,) #1D array of size 100 (latent vector / noise)\n","\n","#Define your generator network \n","#Here we are only using Dense layers. But network can be complicated based\n","#on the application. For example, you can use VGG for super res. GAN.         \n","\n","    model = Sequential()\n","\n","    model.add(Dense(256, input_shape=noise_shape))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Dense(512))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Dense(1024))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(BatchNormalization(momentum=0.8))\n","    \n","    model.add(Dense(np.prod(img_shape), activation='tanh'))\n","    model.add(Reshape(img_shape))\n","\n","    model.summary()\n","\n","    noise = Input(shape=noise_shape)\n","    img = model(noise)    #Generated image\n","\n","    return Model(noise, img)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fIPLW68VBHP0","executionInfo":{"status":"ok","timestamp":1637591216005,"user_tz":-60,"elapsed":330,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"25b8ded7-4b22-4ced-9b4b-6f4bd55a8e3e"},"source":["a=build_generator()"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_3 (Dense)             (None, 256)               25856     \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 256)               0         \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 256)              1024      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_4 (Dense)             (None, 512)               131584    \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 512)               0         \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 512)              2048      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_5 (Dense)             (None, 1024)              525312    \n","                                                                 \n"," leaky_re_lu_5 (LeakyReLU)   (None, 1024)              0         \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 1024)             4096      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_6 (Dense)             (None, 16384)             16793600  \n","                                                                 \n"," reshape (Reshape)           (None, 128, 128, 1)       0         \n","                                                                 \n","=================================================================\n","Total params: 17,483,520\n","Trainable params: 17,479,936\n","Non-trainable params: 3,584\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8qltu5_XB0bm","executionInfo":{"status":"ok","timestamp":1637591256842,"user_tz":-60,"elapsed":29946,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"78f9dc2c-4c02-406d-8d16-490c9877949a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"8wy9PvrzCc9_","executionInfo":{"status":"ok","timestamp":1637591553240,"user_tz":-60,"elapsed":235,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"source":["import numpy as np\n","x=np.load('/content/drive/MyDrive/brain tumor segmentation/asad data/data/appended all/training/appended masks/masks.npy')"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"jq4GzaEaDtEL","executionInfo":{"status":"ok","timestamp":1637591558604,"user_tz":-60,"elapsed":3423,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"source":["image2D = []\n","#mask2D = []\n","for i in range(195):\n","  for j in range(128):\n","    \n","    image2D.append(x[i,:,:,j])\n","    #mask2D.append(y[i,:,:,j])\n","x = np.array(image2D)\n","#y = np.array(mask2D)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vu4Xl6QyEX26","executionInfo":{"status":"ok","timestamp":1637591987164,"user_tz":-60,"elapsed":214,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"source":["def build_discriminator():\n","\n","\n","    model = Sequential()\n","\n","    model.add(Flatten(input_shape=img_shape))\n","    model.add(Dense(512))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dense(256))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.summary()\n","\n","    img = Input(shape=img_shape)\n","    validity = model(img)\n","\n","    return Model(img, validity)\n","#The validity is the Discriminatorâ€™s guess of input being real or not.\n"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"H4XSDCVRDPHq","executionInfo":{"status":"error","timestamp":1637591997210,"user_tz":-60,"elapsed":4034,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"4781860a-9307-4afe-ed3e-7937e2b4ab30"},"source":["def train(epochs, batch_size=128, save_interval=50):\n","\n","    # Load the dataset\n","    X_train=x\n","\n","    # Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n","    #X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n","\n","#Add channels dimension. As the input to our gen and discr. has a shape 28x28x1.\n","    X_train = np.expand_dims(X_train, axis=3) \n","\n","    half_batch = int(batch_size / 2)\n","\n","\n","#We then loop through a number of epochs to train our Discriminator by first selecting\n","#a random batch of images from our true dataset, generating a set of images from our\n","#Generator, feeding both set of images into our Discriminator, and finally setting the\n","#loss parameters for both the real and fake images, as well as the combined loss. \n","    \n","    for epoch in range(epochs):\n","\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","\n","        # Select a random half batch of real images\n","        idx = np.random.randint(0, X_train.shape[0], half_batch)\n","        imgs = X_train[idx]\n","\n","                \n","        noise = np.random.binomial(n=3, p=0.5, size=(half_batch, 100))\n","\n","        # Generate a half batch of fake images\n","        gen_imgs = generator.predict(noise)\n","\n","        # Train the discriminator on real and fake images, separately\n","        #Research showed that separate training is more effective. \n","        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n","        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n","    #take average loss from real and fake images. \n","    #\n","        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n","\n","#And within the same loop we train our Generator, by setting the input noise and\n","#ultimately training the Generator to have the Discriminator label its samples as valid\n","#by specifying the gradient loss.\n","        # ---------------------\n","        #  Train Generator\n","        # ---------------------\n","#Create noise vectors as input for generator. \n","#Create as many noise vectors as defined by the batch size. \n","#Based on normal distribution. Output will be of size (batch size, 100)\n","        noise = np.random.binomial(n=3, p=0.5, size=(half_batch, 100))\n","\n","        # The generator wants the discriminator to label the generated samples\n","        # as valid (ones)\n","        #This is where the genrator is trying to trick discriminator into believing\n","        #the generated image is true (hence value of 1 for y)\n","        valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n","\n","        # Generator is part of combined where it got directly linked with the discriminator\n","        # Train the generator with noise as x and 1 as y. \n","        # Again, 1 as the output as it is adversarial and if generator did a great\n","        #job of folling the discriminator then the output would be 1 (true)\n","        g_loss = combined.train_on_batch(noise, valid_y)\n","\n","\n","#Additionally, in order for us to keep track of our training process, we print the\n","#progress and save the sample image output depending on the epoch interval specified.  \n","# Plot the progress\n","        \n","        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","\n","        # If at save interval => save generated image samples\n","        if epoch % save_interval == 0:\n","            save_imgs(epoch)\n","def save_imgs(epoch):\n","    r, c = 5, 5\n","    noise = np.random.normal(0, 1, (r * c, 100))\n","    gen_imgs = generator.predict(noise)\n","\n","    # Rescale images 0 - 1\n","    gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","    fig, axs = plt.subplots(r, c)\n","    cnt = 0\n","    for i in range(r):\n","        for j in range(c):\n","            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","            axs[i,j].axis('off')\n","            cnt += 1\n","    fig.savefig(\"images/mnist_%d.png\" % epoch)\n","    plt.close()\n","#This function saves our images for us to view\n","\n","\n","##############################################################################\n","\n","#Let us also define our optimizer for easy use later on.\n","#That way if you change your mind, you can change it easily here\n","optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.\n","\n","# Build and compile the discriminator first. \n","#Generator will be trained as part of the combined model, later. \n","#pick the loss function and the type of metric to keep track.                 \n","#Binary cross entropy as we are doing prediction and it is a better\n","#loss function compared to MSE or other. \n","discriminator = build_discriminator()\n","discriminator.compile(loss='binary_crossentropy',\n","    optimizer=optimizer,\n","    metrics=['accuracy'])\n","\n","#build and compile our Discriminator, pick the loss function\n","\n","#SInce we are only generating (faking) images, let us not track any metrics.\n","generator = build_generator()\n","generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","##This builds the Generator and defines the input noise. \n","#In a GAN the Generator network takes noise z as an input to produce its images.  \n","z = Input(shape=(100,))   #Our random input to the generator\n","img = generator(z)\n","\n","#This ensures that when we combine our networks we only train the Generator.\n","#While generator training we do not want discriminator weights to be adjusted. \n","#This Doesn't affect the above descriminator training.     \n","discriminator.trainable = False  \n","\n","#This specifies that our Discriminator will take the images generated by our Generator\n","#and true dataset and set its output to a parameter called valid, which will indicate\n","#whether the input is real or not.  \n","valid = discriminator(img)  #Validity check on the generated image\n","\n","\n","#Here we combined the models and also set our loss function and optimizer. \n","#Again, we are only training the generator here. \n","#The ultimate goal here is for the Generator to fool the Discriminator.  \n","# The combined model  (stacked generator and discriminator) takes\n","# noise as input => generates images => determines validity\n","\n","combined = Model(z, valid)\n","combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","\n","train(epochs=100, batch_size=32, save_interval=10)\n","\n","#Save model for future use to generate fake images\n","#Not tested yet... make sure right model is being saved..\n","#Compare with GAN4\n","\n","generator.save('generator_model.h5')  #Test the model on GAN4_predict...\n","#Change epochs back to 30K\n","                \n","#Epochs dictate the number of backward and forward propagations, the batch_size\n","#indicates the number of training samples per backward/forward propagation, and the\n","#sample_interval specifies after how many epochs we call our sample_image function.\n"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten (Flatten)           (None, 16384)             0         \n","                                                                 \n"," dense_7 (Dense)             (None, 512)               8389120   \n","                                                                 \n"," leaky_re_lu_6 (LeakyReLU)   (None, 512)               0         \n","                                                                 \n"," dense_8 (Dense)             (None, 256)               131328    \n","                                                                 \n"," leaky_re_lu_7 (LeakyReLU)   (None, 256)               0         \n","                                                                 \n"," dense_9 (Dense)             (None, 1)                 257       \n","                                                                 \n","=================================================================\n","Total params: 8,520,705\n","Trainable params: 8,520,705\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_10 (Dense)            (None, 256)               25856     \n","                                                                 \n"," leaky_re_lu_8 (LeakyReLU)   (None, 256)               0         \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 256)              1024      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_11 (Dense)            (None, 512)               131584    \n","                                                                 \n"," leaky_re_lu_9 (LeakyReLU)   (None, 512)               0         \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 512)              2048      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_12 (Dense)            (None, 1024)              525312    \n","                                                                 \n"," leaky_re_lu_10 (LeakyReLU)  (None, 1024)              0         \n","                                                                 \n"," batch_normalization_8 (Batc  (None, 1024)             4096      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_13 (Dense)            (None, 16384)             16793600  \n","                                                                 \n"," reshape_1 (Reshape)         (None, 128, 128, 1)       0         \n","                                                                 \n","=================================================================\n","Total params: 17,483,520\n","Trainable params: 17,479,936\n","Non-trainable params: 3,584\n","_________________________________________________________________\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-e65de4d65e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m#Save model for future use to generate fake images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-e65de4d65e85>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Again, 1 as the output as it is adversarial and if generator did a great\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m#job of folling the discriminator then the output would be 1 (true)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1896\u001b[0m       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n\u001b[1;32m   1897\u001b[0m                                                     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m                                                     class_weight)\n\u001b[0m\u001b[1;32m   1899\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1638\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1640\u001b[0;31m   \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1641\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[1;32m   1656\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 16\n  y sizes: 32\nMake sure all arrays contain the same number of samples."]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"NzgVXjBp-6Pa","executionInfo":{"status":"error","timestamp":1637590355379,"user_tz":-60,"elapsed":2675,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"982b875b-132a-460f-e39d-0ff269834a0b"},"source":["\n","\n","def train(epochs, batch_size=128, save_interval=50):\n","\n","    # Load the dataset\n","    (X_train, _), (_, _) = mnist.load_data()\n","\n","    # Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n","    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n","\n","#Add channels dimension. As the input to our gen and discr. has a shape 28x28x1.\n","    X_train = np.expand_dims(X_train, axis=3) \n","\n","    half_batch = int(batch_size / 2)\n","\n","\n","#We then loop through a number of epochs to train our Discriminator by first selecting\n","#a random batch of images from our true dataset, generating a set of images from our\n","#Generator, feeding both set of images into our Discriminator, and finally setting the\n","#loss parameters for both the real and fake images, as well as the combined loss. \n","    \n","    for epoch in range(epochs):\n","\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","\n","        # Select a random half batch of real images\n","        idx = np.random.randint(0, X_train.shape[0], half_batch)\n","        imgs = X_train[idx]\n","\n"," \n","        noise = np.random.normal(0, 1, (half_batch, 100))\n","\n","        # Generate a half batch of fake images\n","        gen_imgs = generator.predict(noise)\n","\n","        # Train the discriminator on real and fake images, separately\n","        #Research showed that separate training is more effective. \n","        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n","        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n","    #take average loss from real and fake images. \n","    #\n","        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n","\n","#And within the same loop we train our Generator, by setting the input noise and\n","#ultimately training the Generator to have the Discriminator label its samples as valid\n","#by specifying the gradient loss.\n","        # ---------------------\n","        #  Train Generator\n","        # ---------------------\n","#Create noise vectors as input for generator. \n","#Create as many noise vectors as defined by the batch size. \n","#Based on normal distribution. Output will be of size (batch size, 100)\n","        noise = np.random.normal(0, 1, (batch_size, 100)) \n","\n","        # The generator wants the discriminator to label the generated samples\n","        # as valid (ones)\n","        #This is where the genrator is trying to trick discriminator into believing\n","        #the generated image is true (hence value of 1 for y)\n","        valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n","\n","        # Generator is part of combined where it got directly linked with the discriminator\n","        # Train the generator with noise as x and 1 as y. \n","        # Again, 1 as the output as it is adversarial and if generator did a great\n","        #job of folling the discriminator then the output would be 1 (true)\n","        g_loss = combined.train_on_batch(noise, valid_y)\n","\n","\n","#Additionally, in order for us to keep track of our training process, we print the\n","#progress and save the sample image output depending on the epoch interval specified.  \n","# Plot the progress\n","        \n","        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","\n","        # If at save interval => save generated image samples\n","        if epoch % save_interval == 0:\n","            save_imgs(epoch)\n","\n","#when the specific sample_interval is hit, we call the\n","#sample_image function. Which looks as follows.\n","\n","def save_imgs(epoch):\n","    r, c = 5, 5\n","    noise = np.random.normal(0, 1, (r * c, 100))\n","    gen_imgs = generator.predict(noise)\n","\n","    # Rescale images 0 - 1\n","    gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","    fig, axs = plt.subplots(r, c)\n","    cnt = 0\n","    for i in range(r):\n","        for j in range(c):\n","            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","            axs[i,j].axis('off')\n","            cnt += 1\n","    fig.savefig(\"images/mnist_%d.png\" % epoch)\n","    plt.close()\n","#This function saves our images for us to view\n","\n","\n","##############################################################################\n","\n","#Let us also define our optimizer for easy use later on.\n","#That way if you change your mind, you can change it easily here\n","optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.\n","\n","# Build and compile the discriminator first. \n","#Generator will be trained as part of the combined model, later. \n","#pick the loss function and the type of metric to keep track.                 \n","#Binary cross entropy as we are doing prediction and it is a better\n","#loss function compared to MSE or other. \n","discriminator = build_discriminator()\n","discriminator.compile(loss='binary_crossentropy',\n","    optimizer=optimizer,\n","    metrics=['accuracy'])\n","\n","#build and compile our Discriminator, pick the loss function\n","\n","#SInce we are only generating (faking) images, let us not track any metrics.\n","generator = build_generator()\n","generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","##This builds the Generator and defines the input noise. \n","#In a GAN the Generator network takes noise z as an input to produce its images.  \n","z = Input(shape=(100,))   #Our random input to the generator\n","img = generator(z)\n","\n","#This ensures that when we combine our networks we only train the Generator.\n","#While generator training we do not want discriminator weights to be adjusted. \n","#This Doesn't affect the above descriminator training.     \n","discriminator.trainable = False  \n","\n","#This specifies that our Discriminator will take the images generated by our Generator\n","#and true dataset and set its output to a parameter called valid, which will indicate\n","#whether the input is real or not.  \n","valid = discriminator(img)  #Validity check on the generated image\n","\n","\n","#Here we combined the models and also set our loss function and optimizer. \n","#Again, we are only training the generator here. \n","#The ultimate goal here is for the Generator to fool the Discriminator.  \n","# The combined model  (stacked generator and discriminator) takes\n","# noise as input => generates images => determines validity\n","\n","combined = Model(z, valid)\n","combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","\n","train(epochs=100, batch_size=32, save_interval=10)\n","\n","#Save model for future use to generate fake images\n","#Not tested yet... make sure right model is being saved..\n","#Compare with GAN4\n","\n","generator.save('generator_model.h5')  #Test the model on GAN4_predict...\n","#Change epochs back to 30K\n","                \n","#Epochs dictate the number of backward and forward propagations, the batch_size\n","#indicates the number of training samples per backward/forward propagation, and the\n","#sample_interval specifies after how many epochs we call our sample_image function."],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten (Flatten)           (None, 16384)             0         \n","                                                                 \n"," dense (Dense)               (None, 512)               8389120   \n","                                                                 \n"," leaky_re_lu (LeakyReLU)     (None, 512)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," leaky_re_lu_1 (LeakyReLU)   (None, 256)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 257       \n","                                                                 \n","=================================================================\n","Total params: 8,520,705\n","Trainable params: 8,520,705\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_3 (Dense)             (None, 256)               25856     \n","                                                                 \n"," leaky_re_lu_2 (LeakyReLU)   (None, 256)               0         \n","                                                                 \n"," batch_normalization (BatchN  (None, 256)              1024      \n"," ormalization)                                                   \n","                                                                 \n"," dense_4 (Dense)             (None, 512)               131584    \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 512)               0         \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 512)              2048      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_5 (Dense)             (None, 1024)              525312    \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 1024)              0         \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 1024)             4096      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_6 (Dense)             (None, 16384)             16793600  \n","                                                                 \n"," reshape (Reshape)           (None, 128, 128, 1)       0         \n","                                                                 \n","=================================================================\n","Total params: 17,483,520\n","Trainable params: 17,479,936\n","Non-trainable params: 3,584\n","_________________________________________________________________\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-2b57c116f357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;31m#Save model for future use to generate fake images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-2b57c116f357>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Train the discriminator on real and fake images, separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m#Research showed that separate training is more effective.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m#take average loss from real and fake images.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1898\u001b[0m                                                     class_weight)\n\u001b[1;32m   1899\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1900\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 128, 128, 1), found shape=(16, 28, 28, 1)\n"]}]},{"cell_type":"code","metadata":{"id":"Y1AWcqQ4_cFi"},"source":[""],"execution_count":null,"outputs":[]}]}