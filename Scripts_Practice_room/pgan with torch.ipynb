{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pgan with torch.ipynb","provenance":[],"authorship_tag":"ABX9TyO8DpZneLAV0YmlgqV56wGe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"6R-yN2LzuEoe"},"source":["!python "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252},"id":"Ng3Nq_eYsi7Y","executionInfo":{"status":"error","timestamp":1638005334946,"user_tz":-60,"elapsed":1290,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"d84dfef8-6e91-4fa9-f641-6013ea351d46"},"source":["from tqdm import tqdm\n","import numpy as np\n","from PIL import Image\n","import argparse\n","import random\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn, optim\n","from torch.autograd import Variable, grad\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, utils\n","\n","from progan_modules import Generator, Discriminator\n","\n","\n","def accumulate(model1, model2, decay=0.999):\n","    par1 = dict(model1.named_parameters())\n","    par2 = dict(model2.named_parameters())\n","\n","    for k in par1.keys():\n","        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)\n","\n","\n","def imagefolder_loader(path):\n","    def loader(transform):\n","        data = datasets.ImageFolder(path, transform=transform)\n","        data_loader = DataLoader(data, shuffle=True, batch_size=batch_size,\n","                                 num_workers=4)\n","        return data_loader\n","    return loader\n","\n","\n","def sample_data(dataloader, image_size=4):\n","    transform = transforms.Compose([\n","        transforms.Resize(image_size+int(image_size*0.2)+1),\n","        transforms.RandomCrop(image_size),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","    loader = dataloader(transform)\n","\n","    return loader\n","\n","\n","def train(generator, discriminator, init_step, loader, total_iter=600000):\n","    step = init_step # can be 1 = 8, 2 = 16, 3 = 32, 4 = 64, 5 = 128, 6 = 128\n","    data_loader = sample_data(loader, 4 * 2 ** step)\n","    dataset = iter(data_loader)\n","\n","    #total_iter = 600000\n","    total_iter_remain = total_iter - (total_iter//6)*(step-1)\n","\n","    pbar = tqdm(range(total_iter_remain))\n","\n","    disc_loss_val = 0\n","    gen_loss_val = 0\n","    grad_loss_val = 0\n","\n","    from datetime import datetime\n","    import os\n","    date_time = datetime.now()\n","    post_fix = '%s_%s_%d_%d.txt'%(trial_name, date_time.date(), date_time.hour, date_time.minute)\n","    log_folder = 'trial_%s_%s_%d_%d'%(trial_name, date_time.date(), date_time.hour, date_time.minute)\n","    \n","    os.mkdir(log_folder)\n","    os.mkdir(log_folder+'/checkpoint')\n","    os.mkdir(log_folder+'/sample')\n","\n","    config_file_name = os.path.join(log_folder, 'train_config_'+post_fix)\n","    config_file = open(config_file_name, 'w')\n","    config_file.write(str(args))\n","    config_file.close()\n","\n","    log_file_name = os.path.join(log_folder, 'train_log_'+post_fix)\n","    log_file = open(log_file_name, 'w')\n","    log_file.write('g,d,nll,onehot\\n')\n","    log_file.close()\n","\n","    from shutil import copy\n","    copy('train.py', log_folder+'/train_%s.py'%post_fix)\n","    copy('progan_modules.py', log_folder+'/model_%s.py'%post_fix)\n","\n","    alpha = 0\n","    #one = torch.FloatTensor([1]).to(device)\n","    one = torch.tensor(1, dtype=torch.float).to(device)\n","    mone = one * -1\n","    iteration = 0\n","\n","    for i in pbar:\n","        discriminator.zero_grad()\n","\n","        alpha = min(1, (2/(total_iter//6)) * iteration)\n","\n","        if iteration > total_iter//6:\n","            alpha = 0\n","            iteration = 0\n","            step += 1\n","\n","            if step > 6:\n","                alpha = 1\n","                step = 6\n","            data_loader = sample_data(loader, 4 * 2 ** step)\n","            dataset = iter(data_loader)\n","\n","        try:\n","            real_image, label = next(dataset)\n","\n","        except (OSError, StopIteration):\n","            dataset = iter(data_loader)\n","            real_image, label = next(dataset)\n","\n","        iteration += 1\n","\n","        ### 1. train Discriminator\n","        b_size = real_image.size(0)\n","        real_image = real_image.to(device)\n","        label = label.to(device)\n","        real_predict = discriminator(\n","            real_image, step=step, alpha=alpha)\n","        real_predict = real_predict.mean() \\\n","            - 0.001 * (real_predict ** 2).mean()\n","        real_predict.backward(mone)\n","\n","        # sample input data: vector for Generator\n","        gen_z = torch.randn(b_size, input_code_size).to(device)\n","\n","        fake_image = generator(gen_z, step=step, alpha=alpha)\n","        fake_predict = discriminator(\n","            fake_image.detach(), step=step, alpha=alpha)\n","        fake_predict = fake_predict.mean()\n","        fake_predict.backward(one)\n","\n","        ### gradient penalty for D\n","        eps = torch.rand(b_size, 1, 1, 1).to(device)\n","        x_hat = eps * real_image.data + (1 - eps) * fake_image.detach().data\n","        x_hat.requires_grad = True\n","        hat_predict = discriminator(x_hat, step=step, alpha=alpha)\n","        grad_x_hat = grad(\n","            outputs=hat_predict.sum(), inputs=x_hat, create_graph=True)[0]\n","        grad_penalty = ((grad_x_hat.view(grad_x_hat.size(0), -1)\n","                         .norm(2, dim=1) - 1)**2).mean()\n","        grad_penalty = 10 * grad_penalty\n","        grad_penalty.backward()\n","        grad_loss_val += grad_penalty.item()\n","        disc_loss_val += (real_predict - fake_predict).item()\n","\n","        d_optimizer.step()\n","\n","        ### 2. train Generator\n","        if (i + 1) % n_critic == 0:\n","            generator.zero_grad()\n","            discriminator.zero_grad()\n","            \n","            predict = discriminator(fake_image, step=step, alpha=alpha)\n","\n","            loss = -predict.mean()\n","            gen_loss_val += loss.item()\n","\n","\n","            loss.backward()\n","            g_optimizer.step()\n","            accumulate(g_running, generator)\n","\n","        if (i + 1) % 1000 == 0 or i==0:\n","            with torch.no_grad():\n","                images = g_running(torch.randn(5 * 10, input_code_size).to(device), step=step, alpha=alpha).data.cpu()\n","\n","                utils.save_image(\n","                    images,\n","                    f'{log_folder}/sample/{str(i + 1).zfill(6)}.png',\n","                    nrow=10,\n","                    normalize=True,\n","                    range=(-1, 1))\n"," \n","        if (i+1) % 10000 == 0 or i==0:\n","            try:\n","                torch.save(g_running.state_dict(), f'{log_folder}/checkpoint/{str(i + 1).zfill(6)}_g.model')\n","                torch.save(discriminator.state_dict(), f'{log_folder}/checkpoint/{str(i + 1).zfill(6)}_d.model')\n","            except:\n","                pass\n","\n","        if (i+1)%500 == 0:\n","            state_msg = (f'{i + 1}; G: {gen_loss_val/(500//n_critic):.3f}; D: {disc_loss_val/500:.3f};'\n","                f' Grad: {grad_loss_val/500:.3f}; Alpha: {alpha:.3f}')\n","            \n","            log_file = open(log_file_name, 'a+')\n","            new_line = \"%.5f,%.5f\\n\"%(gen_loss_val/(500//n_critic), disc_loss_val/500)\n","            log_file.write(new_line)\n","            log_file.close()\n","\n","            disc_loss_val = 0\n","            gen_loss_val = 0\n","            grad_loss_val = 0\n","\n","            print(state_msg)\n","            #pbar.set_description(state_msg)\n","\n","\n","if __name__ == '__main__':\n","\n","    parser = argparse.ArgumentParser(description='Progressive GAN, during training, the model will learn to generate  images from a low resolution, then progressively getting high resolution ')\n","\n","    parser.add_argument('--path', type=str, help='path of specified dataset, should be a folder that has one or many sub image folders inside')\n","    parser.add_argument('--trial_name', type=str, default=\"test1\", help='a brief description of the training trial')\n","    parser.add_argument('--gpu_id', type=int, default=0, help='0 is the first gpu, 1 is the second gpu, etc.')\n","    parser.add_argument('--lr', type=float, default=0.001, help='learning rate, default is 1e-3, usually dont need to change it, you can try make it bigger, such as 2e-3')\n","    parser.add_argument('--z_dim', type=int, default=128, help='the initial latent vector\\'s dimension, can be smaller such as 64, if the dataset is not diverse')\n","    parser.add_argument('--channel', type=int, default=128, help='determines how big the model is, smaller value means faster training, but less capacity of the model')\n","    parser.add_argument('--batch_size', type=int, default=4, help='how many images to train together at one iteration')\n","    parser.add_argument('--n_critic', type=int, default=1, help='train Dhow many times while train G 1 time')\n","    parser.add_argument('--init_step', type=int, default=1, help='start from what resolution, 1 means 8x8 resolution, 2 means 16x16 resolution, ..., 6 means 256x256 resolution')\n","    parser.add_argument('--total_iter', type=int, default=300000, help='how many iterations to train in total, the value is in assumption that init step is 1')\n","    parser.add_argument('--pixel_norm', default=False, action=\"store_true\", help='a normalization method inside the model, you can try use it or not depends on the dataset')\n","    parser.add_argument('--tanh', default=False, action=\"store_true\", help='an output non-linearity on the output of Generator, you can try use it or not depends on the dataset')\n","    \n","    args = parser.parse_args()\n","\n","    print(str(args))\n","\n","    trial_name = args.trial_name\n","    device = torch.device(\"cuda:%d\"%(args.gpu_id))\n","    input_code_size = args.z_dim\n","    batch_size = args.batch_size\n","    n_critic = args.n_critic\n","\n","    generator = Generator(in_channel=args.channel, input_code_dim=input_code_size, pixel_norm=args.pixel_norm, tanh=args.tanh).to(device)\n","    discriminator = Discriminator(feat_dim=args.channel).to(device)\n","    g_running = Generator(in_channel=args.channel, input_code_dim=input_code_size, pixel_norm=args.pixel_norm, tanh=args.tanh).to(device)\n","    \n","    ## you can directly load a pretrained model here\n","    #generator.load_state_dict(torch.load('./tr checkpoint/150000_g.model'))\n","    #g_running.load_state_dict(torch.load('checkpoint/150000_g.model'))\n","    #discriminator.load_state_dict(torch.load('checkpoint/150000_d.model'))\n","    \n","    g_running.train(False)\n","\n","    g_optimizer = optim.Adam(generator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n","    d_optimizer = optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n","\n","    accumulate(g_running, generator, 0)\n","\n","    loader = imagefolder_loader(args.path)\n","\n","    train(generator, discriminator, args.init_step, loader, args.total_iter)\n"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["usage: ipykernel_launcher.py [-h] [--path PATH] [--trial_name TRIAL_NAME]\n","                             [--gpu_id GPU_ID] [--lr LR] [--z_dim Z_DIM]\n","                             [--channel CHANNEL] [--batch_size BATCH_SIZE]\n","                             [--n_critic N_CRITIC] [--init_step INIT_STEP]\n","                             [--total_iter TOTAL_ITER] [--pixel_norm] [--tanh]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-c663af40-b9a4-4a70-a489-61b9e0867ebd.json\n"]},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]},{"cell_type":"code","metadata":{"id":"6QczVWiqsoFZ","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":111},"executionInfo":{"status":"ok","timestamp":1638005330484,"user_tz":-60,"elapsed":10739,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"4a2ba8d3-5784-4e8c-a55f-a7e8693f2c69"},"source":["from google.colab import files\n","files.upload() "],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-ffe98c06-92c4-457b-a852-4f2739717cdf\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-ffe98c06-92c4-457b-a852-4f2739717cdf\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving progan_modules.py to progan_modules.py\n"]},{"output_type":"execute_result","data":{"text/plain":["{'progan_modules.py': b\"import torch\\r\\nfrom torch import nn\\r\\nfrom torch.nn import functional as F\\r\\n\\r\\nfrom math import sqrt\\r\\n\\r\\n\\r\\nclass EqualLR:\\r\\n    def __init__(self, name):\\r\\n        self.name = name\\r\\n\\r\\n    def compute_weight(self, module):\\r\\n        weight = getattr(module, self.name + '_orig')\\r\\n        fan_in = weight.data.size(1) * weight.data[0][0].numel()\\r\\n\\r\\n        return weight * sqrt(2 / fan_in)\\r\\n\\r\\n    @staticmethod\\r\\n    def apply(module, name):\\r\\n        fn = EqualLR(name)\\r\\n\\r\\n        weight = getattr(module, name)\\r\\n        del module._parameters[name]\\r\\n        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\\r\\n        module.register_forward_pre_hook(fn)\\r\\n\\r\\n        return fn\\r\\n\\r\\n    def __call__(self, module, input):\\r\\n        weight = self.compute_weight(module)\\r\\n        setattr(module, self.name, weight)\\r\\n\\r\\n\\r\\ndef equal_lr(module, name='weight'):\\r\\n    EqualLR.apply(module, name)\\r\\n\\r\\n    return module\\r\\n\\r\\n\\r\\nclass PixelNorm(nn.Module):\\r\\n    def __init__(self):\\r\\n        super().__init__()\\r\\n\\r\\n    def forward(self, input):\\r\\n        return input / torch.sqrt(torch.mean(input ** 2, dim=1, keepdim=True)\\r\\n                                  + 1e-8)\\r\\n\\r\\n\\r\\nclass EqualConv2d(nn.Module):\\r\\n    def __init__(self, *args, **kwargs):\\r\\n        super().__init__()\\r\\n\\r\\n        conv = nn.Conv2d(*args, **kwargs)\\r\\n        conv.weight.data.normal_()\\r\\n        conv.bias.data.zero_()\\r\\n        self.conv = equal_lr(conv)\\r\\n\\r\\n    def forward(self, input):\\r\\n        return self.conv(input)\\r\\n\\r\\n\\r\\nclass EqualConvTranspose2d(nn.Module):\\r\\n    ### additional module for OOGAN usage\\r\\n    def __init__(self, *args, **kwargs):\\r\\n        super().__init__()\\r\\n\\r\\n        conv = nn.ConvTranspose2d(*args, **kwargs)\\r\\n        conv.weight.data.normal_()\\r\\n        conv.bias.data.zero_()\\r\\n        self.conv = equal_lr(conv)\\r\\n\\r\\n    def forward(self, input):\\r\\n        return self.conv(input)\\r\\n\\r\\nclass EqualLinear(nn.Module):\\r\\n    def __init__(self, in_dim, out_dim):\\r\\n        super().__init__()\\r\\n\\r\\n        linear = nn.Linear(in_dim, out_dim)\\r\\n        linear.weight.data.normal_()\\r\\n        linear.bias.data.zero_()\\r\\n\\r\\n        self.linear = equal_lr(linear)\\r\\n\\r\\n    def forward(self, input):\\r\\n        return self.linear(input)\\r\\n\\r\\n\\r\\nclass ConvBlock(nn.Module):\\r\\n    def __init__(self, in_channel, out_channel, kernel_size, padding, kernel_size2=None, padding2=None, pixel_norm=True):\\r\\n        super().__init__()\\r\\n\\r\\n        pad1 = padding\\r\\n        pad2 = padding\\r\\n        if padding2 is not None:\\r\\n            pad2 = padding2\\r\\n\\r\\n        kernel1 = kernel_size\\r\\n        kernel2 = kernel_size\\r\\n        if kernel_size2 is not None:\\r\\n            kernel2 = kernel_size2\\r\\n\\r\\n        convs = [EqualConv2d(in_channel, out_channel, kernel1, padding=pad1)]\\r\\n        if pixel_norm:\\r\\n            convs.append(PixelNorm())\\r\\n        convs.append(nn.LeakyReLU(0.1))\\r\\n        convs.append(EqualConv2d(out_channel, out_channel, kernel2, padding=pad2))\\r\\n        if pixel_norm:\\r\\n            convs.append(PixelNorm())\\r\\n        convs.append(nn.LeakyReLU(0.1))\\r\\n\\r\\n        self.conv = nn.Sequential(*convs)\\r\\n\\r\\n    def forward(self, input):\\r\\n        out = self.conv(input)\\r\\n        return out\\r\\n\\r\\n\\r\\ndef upscale(feat):\\r\\n    return F.interpolate(feat, scale_factor=2, mode='bilinear', align_corners=False)\\r\\n\\r\\nclass Generator(nn.Module):\\r\\n    def __init__(self, input_code_dim=128, in_channel=128, pixel_norm=True, tanh=True):\\r\\n        super().__init__()\\r\\n        self.input_dim = input_code_dim\\r\\n        self.tanh = tanh\\r\\n        self.input_layer = nn.Sequential(\\r\\n            EqualConvTranspose2d(input_code_dim, in_channel, 4, 1, 0),\\r\\n            PixelNorm(),\\r\\n            nn.LeakyReLU(0.1))\\r\\n\\r\\n        self.progression_4 = ConvBlock(in_channel, in_channel, 3, 1, pixel_norm=pixel_norm)\\r\\n        self.progression_8 = ConvBlock(in_channel, in_channel, 3, 1, pixel_norm=pixel_norm)\\r\\n        self.progression_16 = ConvBlock(in_channel, in_channel, 3, 1, pixel_norm=pixel_norm)\\r\\n        self.progression_32 = ConvBlock(in_channel, in_channel, 3, 1, pixel_norm=pixel_norm)\\r\\n        self.progression_64 = ConvBlock(in_channel, in_channel//2, 3, 1, pixel_norm=pixel_norm)\\r\\n        self.progression_128 = ConvBlock(in_channel//2, in_channel//4, 3, 1, pixel_norm=pixel_norm)\\r\\n        self.progression_256 = ConvBlock(in_channel//4, in_channel//4, 3, 1, pixel_norm=pixel_norm)\\r\\n\\r\\n        self.to_rgb_8 = EqualConv2d(in_channel, 3, 1)\\r\\n        self.to_rgb_16 = EqualConv2d(in_channel, 3, 1)\\r\\n        self.to_rgb_32 = EqualConv2d(in_channel, 3, 1)\\r\\n        self.to_rgb_64 = EqualConv2d(in_channel//2, 3, 1)\\r\\n        self.to_rgb_128 = EqualConv2d(in_channel//4, 3, 1)\\r\\n        self.to_rgb_256 = EqualConv2d(in_channel//4, 3, 1)\\r\\n        \\r\\n        self.max_step = 6\\r\\n\\r\\n    def progress(self, feat, module):\\r\\n        out = F.interpolate(feat, scale_factor=2, mode='bilinear', align_corners=False)\\r\\n        out = module(out)\\r\\n        return out\\r\\n\\r\\n    def output(self, feat1, feat2, module1, module2, alpha):\\r\\n        if 0 <= alpha < 1:\\r\\n            skip_rgb = upscale(module1(feat1))\\r\\n            out = (1-alpha)*skip_rgb + alpha*module2(feat2)\\r\\n        else:\\r\\n            out = module2(feat2)\\r\\n        if self.tanh:\\r\\n            return torch.tanh(out)\\r\\n        return out\\r\\n\\r\\n    def forward(self, input, step=0, alpha=-1):\\r\\n        if step > self.max_step:\\r\\n            step = self.max_step\\r\\n\\r\\n        out_4 = self.input_layer(input.view(-1, self.input_dim, 1, 1))\\r\\n        out_4 = self.progression_4(out_4)\\r\\n        out_8 = self.progress(out_4, self.progression_8)\\r\\n        if step==1:\\r\\n            if self.tanh:\\r\\n                return torch.tanh(self.to_rgb_8(out_8))\\r\\n            return self.to_rgb_8(out_8)\\r\\n        \\r\\n        out_16 = self.progress(out_8, self.progression_16)\\r\\n        if step==2:\\r\\n            return self.output( out_8, out_16, self.to_rgb_8, self.to_rgb_16, alpha )\\r\\n        \\r\\n        out_32 = self.progress(out_16, self.progression_32)\\r\\n        if step==3:\\r\\n            return self.output( out_16, out_32, self.to_rgb_16, self.to_rgb_32, alpha )\\r\\n\\r\\n        out_64 = self.progress(out_32, self.progression_64)\\r\\n        if step==4:\\r\\n            return self.output( out_32, out_64, self.to_rgb_32, self.to_rgb_64, alpha )\\r\\n        \\r\\n        out_128 = self.progress(out_64, self.progression_128)\\r\\n        if step==5:\\r\\n            return self.output( out_64, out_128, self.to_rgb_64, self.to_rgb_128, alpha )\\r\\n\\r\\n        out_256 = self.progress(out_128, self.progression_256)\\r\\n        if step==6:\\r\\n            return self.output( out_128, out_256, self.to_rgb_128, self.to_rgb_256, alpha )\\r\\n\\r\\n\\r\\nclass Discriminator(nn.Module):\\r\\n    def __init__(self, feat_dim=128):\\r\\n        super().__init__()\\r\\n\\r\\n        self.progression = nn.ModuleList([ConvBlock(feat_dim//4, feat_dim//4, 3, 1),\\r\\n                                          ConvBlock(feat_dim//4, feat_dim//2, 3, 1),\\r\\n                                          ConvBlock(feat_dim//2, feat_dim, 3, 1),\\r\\n                                          ConvBlock(feat_dim, feat_dim, 3, 1),\\r\\n                                          ConvBlock(feat_dim, feat_dim, 3, 1),\\r\\n                                          ConvBlock(feat_dim, feat_dim, 3, 1),\\r\\n                                          ConvBlock(feat_dim+1, feat_dim, 3, 1, 4, 0)])\\r\\n\\r\\n        self.from_rgb = nn.ModuleList([EqualConv2d(3, feat_dim//4, 1),\\r\\n                                       EqualConv2d(3, feat_dim//4, 1),\\r\\n                                       EqualConv2d(3, feat_dim//2, 1),\\r\\n                                       EqualConv2d(3, feat_dim, 1),\\r\\n                                       EqualConv2d(3, feat_dim, 1),\\r\\n                                       EqualConv2d(3, feat_dim, 1),\\r\\n                                       EqualConv2d(3, feat_dim, 1)])\\r\\n\\r\\n        self.n_layer = len(self.progression)\\r\\n\\r\\n        self.linear = EqualLinear(feat_dim, 1)\\r\\n\\r\\n    def forward(self, input, step=0, alpha=-1):\\r\\n        for i in range(step, -1, -1):\\r\\n            index = self.n_layer - i - 1\\r\\n\\r\\n            if i == step:\\r\\n                out = self.from_rgb[index](input)\\r\\n\\r\\n            if i == 0:\\r\\n                out_std = torch.sqrt(out.var(0, unbiased=False) + 1e-8)\\r\\n                mean_std = out_std.mean()\\r\\n                mean_std = mean_std.expand(out.size(0), 1, 4, 4)\\r\\n                out = torch.cat([out, mean_std], 1)\\r\\n\\r\\n            out = self.progression[index](out)\\r\\n\\r\\n            if i > 0:\\r\\n                # out = F.avg_pool2d(out, 2)\\r\\n                out = F.interpolate(out, scale_factor=0.5, mode='bilinear', align_corners=False)\\r\\n\\r\\n                if i == step and 0 <= alpha < 1:\\r\\n                    # skip_rgb = F.avg_pool2d(input, 2)\\r\\n                    skip_rgb = F.interpolate(input, scale_factor=0.5, mode='bilinear', align_corners=False)\\r\\n                    skip_rgb = self.from_rgb[index + 1](skip_rgb)\\r\\n                    out = (1 - alpha) * skip_rgb + alpha * out\\r\\n\\r\\n        out = out.squeeze(2).squeeze(2)\\r\\n        # print(input.size(), out.size(), step)\\r\\n        out = self.linear(out)\\r\\n\\r\\n        return out\"}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":111},"id":"hq95p4oJuaVd","executionInfo":{"status":"ok","timestamp":1638005492460,"user_tz":-60,"elapsed":6168,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"63359e3b-48ab-46a6-b4df-2795a0740842"},"source":["from google.colab import files\n","files.upload() "],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-4dd885ca-5290-4b88-82b9-4974178726d9\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-4dd885ca-5290-4b88-82b9-4974178726d9\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving train.py to train.py\n"]},{"output_type":"execute_result","data":{"text/plain":["{'train.py': b'from tqdm import tqdm\\r\\nimport numpy as np\\r\\nfrom PIL import Image\\r\\nimport argparse\\r\\nimport random\\r\\n\\r\\nimport torch\\r\\nimport torch.nn.functional as F\\r\\nfrom torch import nn, optim\\r\\nfrom torch.autograd import Variable, grad\\r\\nfrom torch.utils.data import DataLoader\\r\\nfrom torchvision import datasets, transforms, utils\\r\\n\\r\\nfrom progan_modules import Generator, Discriminator\\r\\n\\r\\n\\r\\ndef accumulate(model1, model2, decay=0.999):\\r\\n    par1 = dict(model1.named_parameters())\\r\\n    par2 = dict(model2.named_parameters())\\r\\n\\r\\n    for k in par1.keys():\\r\\n        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)\\r\\n\\r\\n\\r\\ndef imagefolder_loader(path):\\r\\n    def loader(transform):\\r\\n        data = datasets.ImageFolder(path, transform=transform)\\r\\n        data_loader = DataLoader(data, shuffle=True, batch_size=batch_size,\\r\\n                                 num_workers=4)\\r\\n        return data_loader\\r\\n    return loader\\r\\n\\r\\n\\r\\ndef sample_data(dataloader, image_size=4):\\r\\n    transform = transforms.Compose([\\r\\n        transforms.Resize(image_size+int(image_size*0.2)+1),\\r\\n        transforms.RandomCrop(image_size),\\r\\n        transforms.RandomHorizontalFlip(),\\r\\n        transforms.ToTensor(),\\r\\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\r\\n    ])\\r\\n\\r\\n    loader = dataloader(transform)\\r\\n\\r\\n    return loader\\r\\n\\r\\n\\r\\ndef train(generator, discriminator, init_step, loader, total_iter=600000):\\r\\n    step = init_step # can be 1 = 8, 2 = 16, 3 = 32, 4 = 64, 5 = 128, 6 = 128\\r\\n    data_loader = sample_data(loader, 4 * 2 ** step)\\r\\n    dataset = iter(data_loader)\\r\\n\\r\\n    #total_iter = 600000\\r\\n    total_iter_remain = total_iter - (total_iter//6)*(step-1)\\r\\n\\r\\n    pbar = tqdm(range(total_iter_remain))\\r\\n\\r\\n    disc_loss_val = 0\\r\\n    gen_loss_val = 0\\r\\n    grad_loss_val = 0\\r\\n\\r\\n    from datetime import datetime\\r\\n    import os\\r\\n    date_time = datetime.now()\\r\\n    post_fix = \\'%s_%s_%d_%d.txt\\'%(trial_name, date_time.date(), date_time.hour, date_time.minute)\\r\\n    log_folder = \\'trial_%s_%s_%d_%d\\'%(trial_name, date_time.date(), date_time.hour, date_time.minute)\\r\\n    \\r\\n    os.mkdir(log_folder)\\r\\n    os.mkdir(log_folder+\\'/checkpoint\\')\\r\\n    os.mkdir(log_folder+\\'/sample\\')\\r\\n\\r\\n    config_file_name = os.path.join(log_folder, \\'train_config_\\'+post_fix)\\r\\n    config_file = open(config_file_name, \\'w\\')\\r\\n    config_file.write(str(args))\\r\\n    config_file.close()\\r\\n\\r\\n    log_file_name = os.path.join(log_folder, \\'train_log_\\'+post_fix)\\r\\n    log_file = open(log_file_name, \\'w\\')\\r\\n    log_file.write(\\'g,d,nll,onehot\\\\n\\')\\r\\n    log_file.close()\\r\\n\\r\\n    from shutil import copy\\r\\n    copy(\\'train.py\\', log_folder+\\'/train_%s.py\\'%post_fix)\\r\\n    copy(\\'progan_modules.py\\', log_folder+\\'/model_%s.py\\'%post_fix)\\r\\n\\r\\n    alpha = 0\\r\\n    #one = torch.FloatTensor([1]).to(device)\\r\\n    one = torch.tensor(1, dtype=torch.float).to(device)\\r\\n    mone = one * -1\\r\\n    iteration = 0\\r\\n\\r\\n    for i in pbar:\\r\\n        discriminator.zero_grad()\\r\\n\\r\\n        alpha = min(1, (2/(total_iter//6)) * iteration)\\r\\n\\r\\n        if iteration > total_iter//6:\\r\\n            alpha = 0\\r\\n            iteration = 0\\r\\n            step += 1\\r\\n\\r\\n            if step > 6:\\r\\n                alpha = 1\\r\\n                step = 6\\r\\n            data_loader = sample_data(loader, 4 * 2 ** step)\\r\\n            dataset = iter(data_loader)\\r\\n\\r\\n        try:\\r\\n            real_image, label = next(dataset)\\r\\n\\r\\n        except (OSError, StopIteration):\\r\\n            dataset = iter(data_loader)\\r\\n            real_image, label = next(dataset)\\r\\n\\r\\n        iteration += 1\\r\\n\\r\\n        ### 1. train Discriminator\\r\\n        b_size = real_image.size(0)\\r\\n        real_image = real_image.to(device)\\r\\n        label = label.to(device)\\r\\n        real_predict = discriminator(\\r\\n            real_image, step=step, alpha=alpha)\\r\\n        real_predict = real_predict.mean() \\\\\\r\\n            - 0.001 * (real_predict ** 2).mean()\\r\\n        real_predict.backward(mone)\\r\\n\\r\\n        # sample input data: vector for Generator\\r\\n        gen_z = torch.randn(b_size, input_code_size).to(device)\\r\\n\\r\\n        fake_image = generator(gen_z, step=step, alpha=alpha)\\r\\n        fake_predict = discriminator(\\r\\n            fake_image.detach(), step=step, alpha=alpha)\\r\\n        fake_predict = fake_predict.mean()\\r\\n        fake_predict.backward(one)\\r\\n\\r\\n        ### gradient penalty for D\\r\\n        eps = torch.rand(b_size, 1, 1, 1).to(device)\\r\\n        x_hat = eps * real_image.data + (1 - eps) * fake_image.detach().data\\r\\n        x_hat.requires_grad = True\\r\\n        hat_predict = discriminator(x_hat, step=step, alpha=alpha)\\r\\n        grad_x_hat = grad(\\r\\n            outputs=hat_predict.sum(), inputs=x_hat, create_graph=True)[0]\\r\\n        grad_penalty = ((grad_x_hat.view(grad_x_hat.size(0), -1)\\r\\n                         .norm(2, dim=1) - 1)**2).mean()\\r\\n        grad_penalty = 10 * grad_penalty\\r\\n        grad_penalty.backward()\\r\\n        grad_loss_val += grad_penalty.item()\\r\\n        disc_loss_val += (real_predict - fake_predict).item()\\r\\n\\r\\n        d_optimizer.step()\\r\\n\\r\\n        ### 2. train Generator\\r\\n        if (i + 1) % n_critic == 0:\\r\\n            generator.zero_grad()\\r\\n            discriminator.zero_grad()\\r\\n            \\r\\n            predict = discriminator(fake_image, step=step, alpha=alpha)\\r\\n\\r\\n            loss = -predict.mean()\\r\\n            gen_loss_val += loss.item()\\r\\n\\r\\n\\r\\n            loss.backward()\\r\\n            g_optimizer.step()\\r\\n            accumulate(g_running, generator)\\r\\n\\r\\n        if (i + 1) % 1000 == 0 or i==0:\\r\\n            with torch.no_grad():\\r\\n                images = g_running(torch.randn(5 * 10, input_code_size).to(device), step=step, alpha=alpha).data.cpu()\\r\\n\\r\\n                utils.save_image(\\r\\n                    images,\\r\\n                    f\\'{log_folder}/sample/{str(i + 1).zfill(6)}.png\\',\\r\\n                    nrow=10,\\r\\n                    normalize=True,\\r\\n                    range=(-1, 1))\\r\\n \\r\\n        if (i+1) % 10000 == 0 or i==0:\\r\\n            try:\\r\\n                torch.save(g_running.state_dict(), f\\'{log_folder}/checkpoint/{str(i + 1).zfill(6)}_g.model\\')\\r\\n                torch.save(discriminator.state_dict(), f\\'{log_folder}/checkpoint/{str(i + 1).zfill(6)}_d.model\\')\\r\\n            except:\\r\\n                pass\\r\\n\\r\\n        if (i+1)%500 == 0:\\r\\n            state_msg = (f\\'{i + 1}; G: {gen_loss_val/(500//n_critic):.3f}; D: {disc_loss_val/500:.3f};\\'\\r\\n                f\\' Grad: {grad_loss_val/500:.3f}; Alpha: {alpha:.3f}\\')\\r\\n            \\r\\n            log_file = open(log_file_name, \\'a+\\')\\r\\n            new_line = \"%.5f,%.5f\\\\n\"%(gen_loss_val/(500//n_critic), disc_loss_val/500)\\r\\n            log_file.write(new_line)\\r\\n            log_file.close()\\r\\n\\r\\n            disc_loss_val = 0\\r\\n            gen_loss_val = 0\\r\\n            grad_loss_val = 0\\r\\n\\r\\n            print(state_msg)\\r\\n            #pbar.set_description(state_msg)\\r\\n\\r\\n\\r\\nif __name__ == \\'__main__\\':\\r\\n\\r\\n    parser = argparse.ArgumentParser(description=\\'Progressive GAN, during training, the model will learn to generate  images from a low resolution, then progressively getting high resolution \\')\\r\\n\\r\\n    parser.add_argument(\\'--path\\', type=str, help=\\'path of specified dataset, should be a folder that has one or many sub image folders inside\\')\\r\\n    parser.add_argument(\\'--trial_name\\', type=str, default=\"test1\", help=\\'a brief description of the training trial\\')\\r\\n    parser.add_argument(\\'--gpu_id\\', type=int, default=0, help=\\'0 is the first gpu, 1 is the second gpu, etc.\\')\\r\\n    parser.add_argument(\\'--lr\\', type=float, default=0.001, help=\\'learning rate, default is 1e-3, usually dont need to change it, you can try make it bigger, such as 2e-3\\')\\r\\n    parser.add_argument(\\'--z_dim\\', type=int, default=128, help=\\'the initial latent vector\\\\\\'s dimension, can be smaller such as 64, if the dataset is not diverse\\')\\r\\n    parser.add_argument(\\'--channel\\', type=int, default=128, help=\\'determines how big the model is, smaller value means faster training, but less capacity of the model\\')\\r\\n    parser.add_argument(\\'--batch_size\\', type=int, default=4, help=\\'how many images to train together at one iteration\\')\\r\\n    parser.add_argument(\\'--n_critic\\', type=int, default=1, help=\\'train Dhow many times while train G 1 time\\')\\r\\n    parser.add_argument(\\'--init_step\\', type=int, default=1, help=\\'start from what resolution, 1 means 8x8 resolution, 2 means 16x16 resolution, ..., 6 means 256x256 resolution\\')\\r\\n    parser.add_argument(\\'--total_iter\\', type=int, default=300000, help=\\'how many iterations to train in total, the value is in assumption that init step is 1\\')\\r\\n    parser.add_argument(\\'--pixel_norm\\', default=False, action=\"store_true\", help=\\'a normalization method inside the model, you can try use it or not depends on the dataset\\')\\r\\n    parser.add_argument(\\'--tanh\\', default=False, action=\"store_true\", help=\\'an output non-linearity on the output of Generator, you can try use it or not depends on the dataset\\')\\r\\n    \\r\\n    args = parser.parse_args()\\r\\n\\r\\n    print(str(args))\\r\\n\\r\\n    trial_name = args.trial_name\\r\\n    device = torch.device(\"cuda:%d\"%(args.gpu_id))\\r\\n    input_code_size = args.z_dim\\r\\n    batch_size = args.batch_size\\r\\n    n_critic = args.n_critic\\r\\n\\r\\n    generator = Generator(in_channel=args.channel, input_code_dim=input_code_size, pixel_norm=args.pixel_norm, tanh=args.tanh).to(device)\\r\\n    discriminator = Discriminator(feat_dim=args.channel).to(device)\\r\\n    g_running = Generator(in_channel=args.channel, input_code_dim=input_code_size, pixel_norm=args.pixel_norm, tanh=args.tanh).to(device)\\r\\n    \\r\\n    ## you can directly load a pretrained model here\\r\\n    #generator.load_state_dict(torch.load(\\'./tr checkpoint/150000_g.model\\'))\\r\\n    #g_running.load_state_dict(torch.load(\\'checkpoint/150000_g.model\\'))\\r\\n    #discriminator.load_state_dict(torch.load(\\'checkpoint/150000_d.model\\'))\\r\\n    \\r\\n    g_running.train(False)\\r\\n\\r\\n    g_optimizer = optim.Adam(generator.parameters(), lr=args.lr, betas=(0.0, 0.99))\\r\\n    d_optimizer = optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.0, 0.99))\\r\\n\\r\\n    accumulate(g_running, generator, 0)\\r\\n\\r\\n    loader = imagefolder_loader(args.path)\\r\\n\\r\\n    train(generator, discriminator, args.init_step, loader, args.total_iter)\\r\\n'}"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZ3KWAo5vDGO","executionInfo":{"status":"ok","timestamp":1638005499925,"user_tz":-60,"elapsed":663,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"42f4ef4d-f086-4b0c-bc4d-e82aaba6f9c5"},"source":["ls"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["progan_modules.ipynb  progan_modules.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  train.py\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5V2FKg4jvF64","executionInfo":{"status":"ok","timestamp":1638005542055,"user_tz":-60,"elapsed":4968,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"72d8626b-e46d-45a0-8a00-9159b5fb78f3"},"source":["!python train.py"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(batch_size=4, channel=128, gpu_id=0, init_step=1, lr=0.001, n_critic=1, path=None, pixel_norm=False, tanh=False, total_iter=300000, trial_name='test1', z_dim=128)\n","Traceback (most recent call last):\n","  File \"train.py\", line 229, in <module>\n","    generator = Generator(in_channel=args.channel, input_code_dim=input_code_size, pixel_norm=args.pixel_norm, tanh=args.tanh).to(device)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 899, in to\n","    return self._apply(convert)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 570, in _apply\n","    module._apply(fn)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 570, in _apply\n","    module._apply(fn)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 570, in _apply\n","    module._apply(fn)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 593, in _apply\n","    param_applied = fn(param)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 897, in convert\n","    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\", line 214, in _lazy_init\n","    torch._C._cuda_init()\n","RuntimeError: No CUDA GPUs are available\n"]}]},{"cell_type":"code","metadata":{"id":"Hxy8fn4CvPZr"},"source":[""],"execution_count":null,"outputs":[]}]}