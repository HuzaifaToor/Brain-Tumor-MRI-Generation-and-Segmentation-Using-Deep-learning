{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2:1 complete model with data genrator.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1bmbApqM5Ct9xhUrZakpd6eZUxIAgmVOy","authorship_tag":"ABX9TyPXr/rbPVZuhzukYBg+mY7v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Y0sEwWzyjxhw","executionInfo":{"status":"ok","timestamp":1640018047742,"user_tz":-60,"elapsed":239,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"outputs":[],"source":["import numpy as np\n","import nibabel as nib\n","import glob\n","#from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["from keras.models import Model\n","from keras.layers import Input\n","from keras.layers.core import Activation, Reshape, Dense\n","from keras.layers.convolutional import Convolution2D\n","from keras.layers import BatchNormalization\n","#from keras import layers\n","from keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import numpy as np\n","import nibabel as nib\n","import glob\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.optimizers import Adam"],"metadata":{"id":"F4qK07G_kfx3","executionInfo":{"status":"ok","timestamp":1640018050910,"user_tz":-60,"elapsed":2138,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\n","def iou(y_true, y_pred, smooth = 100):\n","    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n","    sum_ = K.sum(K.square(y_true), axis = -1) + K.sum(K.square(y_pred), axis=-1)\n","    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n","    return jac\n","\n","def dice_coef(y_true, y_pred, smooth = 100):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","\n","def precision(y_true, y_pred):\n","    '''Calculates the precision, a metric for multi-label classification of\n","    how many selected items are relevant.\n","    '''\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","def recall(y_true, y_pred):\n","    '''Calculates the recall, a metric for multi-label classification of\n","    how many relevant items are selected.\n","    '''\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","def accuracy(y_true, y_pred):\n","    '''Calculates the mean accuracy rate across all predictions for binary\n","    classification problems.\n","    '''\n","    return K.mean(K.equal(y_true, K.round(y_pred)))\n"],"metadata":{"id":"AJ6zXxlZkgKs","executionInfo":{"status":"ok","timestamp":1640018050911,"user_tz":-60,"elapsed":13,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from keras.models import Model\n","from keras.layers import Input\n","from keras.layers.core import Activation, Reshape, Dense\n","from keras.layers.convolutional import Convolution2D\n","from keras.layers import BatchNormalization\n","\n","#from layers import MaxPoolingWithArgmax2D, MaxUnpooling2D\n","\n","def segnet(\n","        input_shape,\n","        n_labels,\n","        kernel=3,\n","        pool_size=(2, 2),\n","        output_mode=\"softmax\"):\n","    # encoder\n","    inputs = Input(shape=input_shape)\n","    #inputs = tf.convert_to_tensor(inputs)\n","    conv_1 = Convolution2D(64, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal', name=\"block1_conv1\")(inputs)\n","    conv_1 = BatchNormalization()(conv_1)\n","    conv_1 = Activation(\"relu\")(conv_1)\n","    conv_2 = Convolution2D(64, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal', name=\"block1_conv2\")(conv_1)\n","    conv_2 = BatchNormalization()(conv_2)\n","    conv_2 = Activation(\"relu\")(conv_2)\n","    pool_1 = MaxPooling2D(pool_size, name=\"block1_pool\")(conv_2)\n","\n","    conv_3 = Convolution2D(128, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal', name=\"block2_conv1\")(pool_1)\n","    conv_3 = BatchNormalization()(conv_3)\n","    conv_3 = Activation(\"relu\")(conv_3)\n","    conv_4 = Convolution2D(128, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal',  name=\"block2_conv2\")(conv_3)\n","    conv_4 = BatchNormalization()(conv_4)\n","    conv_4 = Activation(\"relu\")(conv_4)\n","\n","    pool_2 = MaxPooling2D(pool_size, name=\"block2_pool\")(conv_4)\n","\n","    conv_5 = Convolution2D(256, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal', name=\"block3_conv1\")(pool_2)\n","    conv_5 = BatchNormalization()(conv_5)\n","    conv_5 = Activation(\"relu\")(conv_5)\n","    conv_6 = Convolution2D(256, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal', name=\"block3_conv2\")(conv_5)\n","    conv_6 = BatchNormalization()(conv_6)\n","    conv_6 = Activation(\"relu\")(conv_6)\n","    conv_7 = Convolution2D(256, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal', name=\"block3_conv3\")(conv_6)\n","    conv_7 = BatchNormalization()(conv_7)\n","    conv_7 = Activation(\"relu\")(conv_7)\n","\n","    pool_3 = MaxPooling2D(pool_size, name=\"block3_pool\")(conv_7)\n","\n","    conv_8 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal', name=\"block4_conv1\")(pool_3)\n","    conv_8 = BatchNormalization()(conv_8)\n","    conv_8 = Activation(\"relu\")(conv_8)\n","    conv_9 = Convolution2D(512, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal', name=\"block4_conv2\")(conv_8)\n","    conv_9 = BatchNormalization()(conv_9)\n","    conv_9 = Activation(\"relu\")(conv_9)\n","    conv_10 = Convolution2D(512, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal', name=\"block4_conv3\")(conv_9)\n","    conv_10 = BatchNormalization()(conv_10)\n","    conv_10 = Activation(\"relu\")(conv_10)\n","\n","    pool_4 = MaxPooling2D(pool_size, name=\"block4_pool\")(conv_10)\n","\n","    conv_11 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal', name=\"block5_conv1\")(pool_4)\n","    conv_11 = BatchNormalization()(conv_11)\n","    conv_11 = Activation(\"relu\")(conv_11)\n","    conv_12 = Convolution2D(512, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal',  name=\"block5_conv2\")(conv_11)\n","    conv_12 = BatchNormalization()(conv_12)\n","    conv_12 = Activation(\"relu\")(conv_12)\n","    conv_13 = Convolution2D(512, (kernel, kernel), padding=\"same\" , kernel_initializer='he_normal',  name=\"block5_conv3\")(conv_12)\n","    conv_13 = BatchNormalization()(conv_13)\n","    conv_13 = Activation(\"relu\")(conv_13)\n","\n","    pool_5 = MaxPooling2D(pool_size, name=\"block5_pool\")(conv_13)\n","   \n","    # decoder\n","    unpool_1 = UpSampling2D(pool_size)(pool_5)\n","\n","    conv_14 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(unpool_1)\n","    conv_14 = BatchNormalization()(conv_14)\n","    conv_14 = Activation(\"relu\")(conv_14)\n","    conv_15 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_14)\n","    conv_15 = BatchNormalization()(conv_15)\n","    conv_15 = Activation(\"relu\")(conv_15)\n","    conv_16 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_15)\n","    conv_16 = BatchNormalization()(conv_16)\n","    conv_16 = Activation(\"relu\")(conv_16)\n","\n","    unpool_2 = UpSampling2D(pool_size)(conv_16)\n","\n","    conv_17 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(unpool_2)\n","    conv_17 = BatchNormalization()(conv_17)\n","    conv_17 = Activation(\"relu\")(conv_17)\n","    conv_18 = Convolution2D(512, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_17)\n","    conv_18 = BatchNormalization()(conv_18)\n","    conv_18 = Activation(\"relu\")(conv_18)\n","    conv_19 = Convolution2D(256, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_18)\n","    conv_19 = BatchNormalization()(conv_19)\n","    conv_19 = Activation(\"relu\")(conv_19)\n","\n","    unpool_3 = UpSampling2D(pool_size)(conv_19)\n","\n","    conv_20 = Convolution2D(256, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(unpool_3)\n","    conv_20 = BatchNormalization()(conv_20)\n","    conv_20 = Activation(\"relu\")(conv_20)\n","    conv_21 = Convolution2D(256, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_20)\n","    conv_21 = BatchNormalization()(conv_21)\n","    conv_21 = Activation(\"relu\")(conv_21)\n","    conv_22 = Convolution2D(128, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_21)\n","    conv_22 = BatchNormalization()(conv_22)\n","    conv_22 = Activation(\"relu\")(conv_22)\n","\n","    unpool_4 = UpSampling2D(pool_size)(conv_22)\n","\n","    conv_23 = Convolution2D(128, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(unpool_4)\n","    conv_23 = BatchNormalization()(conv_23)\n","    conv_23 = Activation(\"relu\")(conv_23)\n","    conv_24 = Convolution2D(64, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(conv_23)\n","    conv_24 = BatchNormalization()(conv_24)\n","    conv_24 = Activation(\"relu\")(conv_24)\n","\n","    unpool_5 = UpSampling2D(pool_size)(conv_24)\n","    #pool, ind, output_shape, batch_size, name=None\n","\n","    conv_25 = Convolution2D(64, (kernel, kernel), padding=\"same\", kernel_initializer='he_normal')(unpool_5)\n","    conv_25 = BatchNormalization()(conv_25)\n","    conv_25 = Activation(\"relu\")(conv_25)\n","\n","    conv_26 = Convolution2D(n_labels, (1, 1), padding=\"same\", kernel_initializer='he_normal')(conv_25)\n","    conv_26 = BatchNormalization()(conv_26)\n","\n","    conv_26 = Reshape((128,128,4))(conv_26)\n","    #conv_26 = Dense(4)(conv_26)\n","    outputs = Activation(output_mode)(conv_26)\n","    print(\"Build decoder done..\")\n","    #pred = Reshape((128,128,128))(outputs)\n","    model = Model(inputs=inputs, outputs=outputs, name=\"SegNet\")\n","\n","    return model\n","\n","\n","model = segnet(input_shape=(128,128,1), n_labels=4)\n","\n","model.summary()\n","print(model.input_shape)\n","print(model.output_shape)"],"metadata":{"id":"AEFrpSp8kjXQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##############################################################\n","#Define the image generators for training and validation\n","\n","train_img_dir = \"/content/drive/MyDrive/Brain_tumor_Segmentation/asad data/results/final Results all modility/results/complete results/results for combined with different ratios x_test real/2:1/individual files/Sep_imgs_train/\"\n","train_mask_dir = \"/content/drive/MyDrive/Brain_tumor_Segmentation/asad data/results/final Results all modility/results/complete results/results for combined with different ratios x_test real/2:1/individual files/Sep_msks_train/\"\n","\n","val_img_dir = \"/content/drive/MyDrive/Brain_tumor_Segmentation/asad data/results/final Results all modility/results/complete results/results for combined with different ratios x_test real/2:1/individual files/Sep_imgs_val/\"\n","val_mask_dir = \"/content/drive/MyDrive/Brain_tumor_Segmentation/asad data/results/final Results all modility/results/complete results/results for combined with different ratios x_test real/2:1/individual files/Sep_msks_val/\"\n","\n","import os\n","import numpy as np\n","\n","\n","\n","train_img_list=os.listdir(train_img_dir)\n","train_mask_list = os.listdir(train_mask_dir)\n","\n","val_img_list=os.listdir(val_img_dir)\n","val_mask_list = os.listdir(val_mask_dir)"],"metadata":{"id":"POrI38Fhkrum","executionInfo":{"status":"ok","timestamp":1640018074643,"user_tz":-60,"elapsed":12461,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","def load_img(img_dir, img_list):\n","    images=[]\n","    for i, image_name in enumerate(img_list):    \n","        if (image_name.split('.')[1] == 'npy'):\n","            \n","            image = np.load(img_dir+image_name)\n","                      \n","            images.append(image)\n","    images = np.array(images)\n","    \n","    return(images)\n","\n","\n","\n","\n","def imageLoader(img_dir, img_list, mask_dir, mask_list, batch_size):\n","\n","    L = len(img_list)\n","\n","    #keras needs the generator infinite, so we will use while true  \n","    while True:\n","\n","        batch_start = 0\n","        batch_end = batch_size\n","\n","        while batch_start < L:\n","            limit = min(batch_end, L)\n","                       \n","            X = load_img(img_dir, img_list[batch_start:limit])\n","            Y = load_img(mask_dir, mask_list[batch_start:limit])\n","\n","            yield (X,Y) #a tuple with two numpy arrays with batch_size samples     \n","\n","            batch_start += batch_size   \n","            batch_end += batch_size"],"metadata":{"id":"C3M0_rUErbvO","executionInfo":{"status":"ok","timestamp":1640018076146,"user_tz":-60,"elapsed":216,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","\n","train_img_datagen = imageLoader(train_img_dir, train_img_list, \n","                                train_mask_dir, train_mask_list, batch_size)\n","\n","val_img_datagen = imageLoader(val_img_dir, val_img_list, \n","                                val_mask_dir, val_mask_list, batch_size)"],"metadata":{"id":"gR4NRgmMlSbc","executionInfo":{"status":"ok","timestamp":1640018079457,"user_tz":-60,"elapsed":241,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["img, msk = train_img_datagen.__next__()\n"],"metadata":{"id":"YJbmVP9JnocK","executionInfo":{"status":"ok","timestamp":1640017180159,"user_tz":-60,"elapsed":30511,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["img.shape, msk.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QJGvVMDWn8Kx","executionInfo":{"status":"ok","timestamp":1640017180160,"user_tz":-60,"elapsed":31,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"8de28915-5afc-42ad-e827-87de74df11c2"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((32, 128, 128, 1), (32, 128, 128, 4))"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import glob\n","import numpy as np"],"metadata":{"id":"GkDhpymGCI8R","executionInfo":{"status":"ok","timestamp":1640018082707,"user_tz":-60,"elapsed":371,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["!pip install keras_applications\n","!pip install classification-models-3D\n","!pip install efficientnet-3D\n","!pip install segmentation-models-3D"],"metadata":{"id":"jfiYIStZmHOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###########################################################################\n","#Define loss, metrics and optimizer to be used for training\n","wt0, wt1, wt2, wt3 = 0.25,0.25,0.25,0.25\n","import segmentation_models_3D as sm\n","dice_loss = sm.losses.DiceLoss(class_weights=np.array([wt0, wt1, wt2, wt3])) \n","focal_loss = sm.losses.CategoricalFocalLoss()\n","total_loss = dice_loss + (1 * focal_loss)\n","\n","metrics = ['accuracy', sm.metrics.IOUScore(threshold=0.5)]\n","\n","LR = 0.001\n","from tensorflow.keras.optimizers import Adam\n","optim = Adam(LR)\n","#######################################################################\n","#Fit the model \n","\n","steps_per_epoch = len(train_img_list)//batch_size\n","val_steps_per_epoch = len(val_img_list)//batch_size\n","\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0IcUG8QvmQSw","executionInfo":{"status":"ok","timestamp":1640018103191,"user_tz":-60,"elapsed":245,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"6242ea57-fc48-448b-c949-006109108198"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Segmentation Models: using `tf.keras` framework.\n"]}]},{"cell_type":"code","source":["print(model.input_shape)\n","print(model.output_shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o69KpAuRmvPp","executionInfo":{"status":"ok","timestamp":1640018110148,"user_tz":-60,"elapsed":322,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}},"outputId":"8ea28811-dd7f-4e67-b120-4dcb63e1f878"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 128, 128, 1)\n","(None, 128, 128, 4)\n"]}]},{"cell_type":"code","source":["model.compile(optimizer= Adam(learning_rate=0.0001), loss= [\"categorical_crossentropy\"]\n","                  , metrics=['acc'])\n"],"metadata":{"id":"3IK_Z8t9nXiF","executionInfo":{"status":"ok","timestamp":1640018111763,"user_tz":-60,"elapsed":252,"user":{"displayName":"Huzaifa Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLYtmrg9ffTuIoG7yviU5wA27X_NroIWVZbOUj=s64","userId":"08238025780309927389"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["\n","hist = model.fit(train_img_datagen, epochs= 40, batch_size= 10, validation_data= val_img_datagen, verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJDtoS11sK5S","outputId":"76ee682e-ded2-453b-891a-1693159d756c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/40\n","    464/Unknown - 2003s 4s/step - loss: 1.3521 - acc: 0.6769"]}]},{"cell_type":"code","source":["\n","'''history=model.fit(train_img_datagen,\n","          steps_per_epoch=steps_per_epoch,\n","          epochs=40,\n","          verbose=1,\n","          validation_data=val_img_datagen,\n","          validation_steps=val_steps_per_epoch,\n","          )\n","'''"],"metadata":{"id":"vAINcGZEmh10"},"execution_count":null,"outputs":[]}]}